{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prerequisite-danish",
   "metadata": {},
   "source": [
    "# Perceptroni Multi-Strato\n",
    "\n",
    "I _Perceptroni Multi-Strato_ (_Multi-Layer Perceptron_, **MLP**) sono l'esempio tipico di _Rete Neurale_ (_Neural Network:, **NN**).\n",
    "\n",
    "## MLP in Breve\n",
    "\n",
    "Gli MLP sono un particolare tipo di _feedforward_ NN (cioè senza ricorsività/cicli nella sua struttura) caratterizzato da una sequenza di _strati completamente connessi_ (_fully-connected layers_, *FC Layers*).\n",
    "\n",
    "------------------------------\n",
    "**\"DEFINIZIONE\"** _(Funzione caratterizzante un Fully-Connected Layer)_**:**\n",
    "\n",
    "Sia $L$ un FC layer di $m\\in\\mathbb{N}$ unità con funzione di attivazione $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ e sia $L$ (completamente) connesso con un altro livello di $n$ unità. Il layer $L$ è quindi caratterizzato dalla funzione $\\mathcal{L}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\boldsymbol{x}) := \\boldsymbol{\\sigma}\\left(W\\boldsymbol{x} + \\boldsymbol{b}\\right)\\,,\\quad \\forall \\ \\boldsymbol{x}\\in\\mathbb{R}^n\\,,\n",
    "\\end{equation}\n",
    "dove:\n",
    "- $W\\in\\mathbb{R}^{m\\times n}$ è la matrice dei pesi del livello $L$;\n",
    "- $\\boldsymbol{b}\\in\\mathbb{R}^m$ è il vettore dei bias\n",
    "- $\\boldsymbol{\\sigma}:\\mathbb{R}^m\\rightarrow\\mathbb{R}^m$ è una funzione vettoriale che applica elemento-per-elemento la funzione $\\sigma$.\n",
    "------------------------------\n",
    "\n",
    "------------------------------\n",
    "**\"DEFINIZIONE\"** _(Input Layer)_**:**\n",
    "\n",
    "Un _Input Layer_ di $n\\in\\mathbb{N}$ unità è un layer che \"_legge_\" vettori di $\\mathbb{R}^n$ e li \"_invia_\" ai layer successivi con lui connessi.\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-carolina",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "**\"DEFINIZIONE\"** _(Funzione caratterizzante un Multi-Layer Perceptron)_**:**\n",
    "\n",
    "Sia dato un MLP costituito da un input layer $L^{(0)}$ di $n\\in\\mathbb{N}$ unità, seguito da una sequenza di FC layers $L^{(1)},\\ldots ,L^{(H)}, L^{(H+1)}$ connessi uno dopo l'altro. In particolare, i layer $L^{(1)},\\ldots ,L^{(H)}$ sono definiti _strati nascosti_ (_hidden layers_) mentre $L^{(H+1)}$ è definito _strato di output_ (_output layer_).\n",
    "\n",
    "L'MLP in questione è quindi rappresentato da una funzione $\\hat{\\boldsymbol{F}}:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\boldsymbol{x}) : \\mathbb{R}^n \\xrightarrow[]{\\mathcal{L}^{(1)}} \\mathbb{R}^{n_1} \\xrightarrow[]{\\mathcal{L}^{(2)}} \\cdots \\xrightarrow[]{\\mathcal{L}^{(H)}} \\mathbb{R}^{n_H}\\xrightarrow[]{\\mathcal{L}^{(H+1)}}\\mathbb{R}^m\n",
    "\\end{equation}\n",
    "\n",
    "e in particolare \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\boldsymbol{x}) = \\boldsymbol{\\sigma}^{(H+1)}\\left( W^{(H+1)}\\boldsymbol{\\sigma}^{(H)}\\left(\\cdots \\left( W^{(2)}\\boldsymbol{\\sigma}^{(1)}\\left(W^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right)+\\boldsymbol{b}^{(2)}\\right) \\cdots\\right)  + \\boldsymbol{b}^{(H+1)}\\right)\\,,\\quad \\forall \\ \\boldsymbol{x}\\in\\mathbb{R}^n\\,,\n",
    "\\end{equation}\n",
    "\n",
    "dove:\n",
    "- $W^{(h)}$, $\\boldsymbol{b}^{(h)}$, $\\sigma^{(h)}$, sono rispettivamente i pesi, i bias e la funzione di attivazione del layer $h$-esimo, per ogni $h=1,\\ldots , H+1$;\n",
    "- $n_h\\in\\mathbb{N}$ è il numero di unità del layer $h$-esimo, per ogni $h=1,\\ldots ,H$;\n",
    "- $m\\in\\mathbb{N}$ è il numero di unità dell'output layer $L^{(H+1)}$.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "-----------------\n",
    "\n",
    "**OSSERVAZIONE** _($\\hat{\\boldsymbol{F}}$ come Funzione Parametrica)_**:** \n",
    "\n",
    "La funzione $\\hat{\\boldsymbol{F}}$ caratterizzante un MLP è, come le funzioni di tutti gli altri algoritmi di Machine Learning (ML) una funzione parametrica con parametri i pesi $W^{(h)}$ ed i bias $\\boldsymbol{b}^{(h)}$. Indicando con $\\boldsymbol{w}$ il vettore ottenuto dalla concatenazione delle vettorializzazioni di tutti i pesi $W^{(h)}$ e di tutti i bias $\\boldsymbol{b}^{(h)}$, possiamo esplicitare la dipendenza dell'MLP da questi parametri con la seguente notazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\ \\cdot \\ \\,;\\, \\boldsymbol{w})\\quad \\text{oppure} \\quad \\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}(\\ \\cdot \\ )\n",
    "\\end{equation}\n",
    "\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-conspiracy",
   "metadata": {},
   "source": [
    "## Metodi di Discesa del Gradiente\n",
    "\n",
    "\n",
    "I metodi di discesa del gradiente sono _metodi numerici iterativi_ per la minimizzazione (massimizzazione) di funzioni $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. \n",
    "\n",
    "Le funzioni da minimizzare sono generalmente definite indifferentemente come \"_funzioni obiettivo_\" (_objective functions_), \"_funzioni di costo_\" (_cost functions_) o \"_funzioni di perdita_\" (_loss functions_).\n",
    "\n",
    "Questi metodi si basano sull'osservazione che $-\\nabla f(\\boldsymbol{x}_0)$ è la _direzione di più ripida discesa_ per $f$ nel punto $\\boldsymbol{x}_0$ (analogamente, $\\nabla f(\\boldsymbol{x}_0)$ è la direzione di più ripida ascesa).\n",
    "\n",
    "------------------\n",
    "\n",
    "**DEFINIZIONE** _(Direzione di Discesa)_**:**\n",
    "\n",
    "Un vettore $\\boldsymbol{p}\\in\\mathbb{R}^n$ è una _direzione di discesa_ per la funzione $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ in $\\boldsymbol{x}_0\\in\\mathbb{R}^n$ se esiste $\\alpha^*\\in\\mathbb{R}^+$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\boldsymbol{x}_0)\\geq f(\\boldsymbol{x}_0 + \\alpha\\boldsymbol{p})\\,,\\quad \\forall \\ \\alpha\\in [0, \\alpha^*]\n",
    "\\end{equation}\n",
    "\n",
    "------------------\n",
    "\n",
    "------------------\n",
    "\n",
    "**METODO** _(Steepest Descent - a grandi linee...)_**:**\n",
    "\n",
    "Il _metodo di più ripida discesa_ (_steepest descent method_) è un metodo di discesa del gradiente per la minimizzazione di funzioni $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, cioè per trovare la soluzione al problema\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\boldsymbol{x}\\in\\mathbb{R}^n} f(\\boldsymbol{x})\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Dato quindi un punto di partenza _arbitrario_ $\\boldsymbol{x}_0\\in\\mathbb{R}^n$, abbiamo che il passo $k$-esimo del metodo è definito dalla seguente operazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_{k} - \\alpha_{k}\\nabla f(\\boldsymbol{x}_k)\\,, \\quad \\forall \\ k\\geq 0\\,,\n",
    "\\end{equation}\n",
    "\n",
    "con $\\alpha_k\\in\\mathbb{R}^+$ è un _fattore di moltiplicazione del passo di discesa_ (nei casi più semplici, $\\alpha_k=\\alpha$ costante per ogni $k$).\n",
    "\n",
    "\n",
    "------------------\n",
    "\n",
    "------------------\n",
    "\n",
    "**LAVORARE CON IL METODO DI PIU' RIPIDA DISCESA** _(Proprietà e Problemi)_**:**\n",
    "\n",
    "1. Sotto specifiche ipotesi di regolarità per $f$ e/o opportune scelte di $\\boldsymbol{x}_0$ ed $\\{\\alpha_k\\}_{k\\in\\mathbb{N}}$, il metodo _CONVERGE_ ad un minimo _LOCALE_ della funzione;\n",
    "\n",
    "2. Se non si hanno informazioni sulle proprietà di $f$, la convergenza del metodo è altamente influenzata dalla scelta del punto di partenza $\\boldsymbol{x}_0$ e dalla successione $\\{\\alpha_k\\}_{k\\in\\mathbb{N}}$ di moltiplicatori del passo. Il metodo potrebbe quindi CONVERGERE, DIVERGERE, \"OSCILLLARE\";\n",
    "\n",
    "3. Vanno stabiliti opportuni criteri di arresto (altrimenti il metodo contrinuerebbe all'infinito). Anche i parametri caratterizzanti i criteri di arresto influenzano la convergenza del metodo.\n",
    "\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-failure",
   "metadata": {},
   "source": [
    "\n",
    "## Addestramento di un MLP - in Breve\n",
    "\n",
    "L'addestramento di un MLP (e di una NN in generale) richiede sia conoscenze teoriche dei metodi di ottimizzazione numerica che una certa sensibilità pratica/empirica per la scelta dei parametri che li caratterizzano. \n",
    "\n",
    "------------------\n",
    "**ATTENZIONE** _(Aggiunta di una Componente Stocastica)_**:** \n",
    "\n",
    "Si deve inoltre considerare che l'addestramento di NN introduce delle componenti _stocastiche_ nei metodi numerici sopra citati. In altre parole, non si minimizza una generica funzione di costo \"_fissata_\", ma una funzione di costo caratterizzata dai campioni del mio dataset che uso per l'addestramento.\n",
    "\n",
    "------------------\n",
    "\n",
    "### Funzioni di Loss Rispetto a \"Gruppi\" di Dati\n",
    "\n",
    "\n",
    "Data una funzione target $\\boldsymbol{F}$, per addestrare un MLP con funzione parametrica $\\hat{\\boldsymbol{F}}_\\boldsymbol{w}$ ad apprendere $\\boldsymbol{F}$, vorremmo _idealmente_ risolvere il problema\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\boldsymbol{w}} \\left\\lbrace \\mathrm{Loss}(\\boldsymbol{w}):= \\sum_{\\boldsymbol{x}\\in\\mathbb{R}^n}|\\boldsymbol{F}(\\boldsymbol{x}) - \\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}(\\boldsymbol{x})|\\right\\rbrace\\,;\n",
    "\\end{equation}\n",
    "in altre parole, vorremmo trovare $\\boldsymbol{w}^*$ tale che $\\mathrm{Loss}(\\boldsymbol{w}^*)=0$.\n",
    "\n",
    "**ATTENZIONE:** NOTARE CHE NEL PROBLEMA DI MINIMO LE VARIABILI SONO I PARAMETRI $\\boldsymbol{w}$, NON LE $\\boldsymbol{x}$!!\n",
    "\n",
    "Ovviamente, non disponendo degli infiniti punti $\\boldsymbol{x}\\in\\mathbb{R}^n$ (e spesso neanche della funzione target $\\boldsymbol{F}$, ma solo delle valutazioni $\\boldsymbol{y}=\\boldsymbol{F}(\\boldsymbol{x})$), il problema ideale sopra indicato va ri-adattato alla pratica, cioè ai dati disponibili contenuti nel training set\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{T}=\\{ (\\boldsymbol{x}_1,\\boldsymbol{y}_1),\\ldots ,  (\\boldsymbol{x}_T,\\boldsymbol{y}_T) \\}\\,.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-muslim",
   "metadata": {},
   "source": [
    "#### Approccio 1: Niente Stocasticità\n",
    "\n",
    "L'idea più semplice per addestrare un MLP può essere quindi quello di definire una funzione di loss \"troncando\" quella del problema ideale, cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\sum_{i=1}^T \\underbrace{|\\boldsymbol{y}_i - \\hat{\\boldsymbol{F}}(\\boldsymbol{x}_i)| }_{\\text{err. el.-per-el.}} \\,.\n",
    "\\end{equation}\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"fissata\" e può essere risolto tramite il metodo di più ripida discesa (o sue varianti); avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 1)_**:**\n",
    "\n",
    "L'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è definito _EPOCA DI ADDESTRAMENTO_, nella terminologia delle NN. Il più semplice dei criteri di arresto è quindi quello di inserire un numero massimo di epoche da eseguire.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, nella terminologia delle NN, è definito _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "  \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-helen",
   "metadata": {},
   "source": [
    "#### Approccio 2: Stocasticità \"Pura\"\n",
    "\n",
    "Poiché addestrare l'MLP rispetto ad un errore \"aggregato\" rispetto a tutto il training set potrebbe dare risultati poco sensibili ai dettagli della funzione target $\\boldsymbol{F}$, l'idea è quella di introdurre della stocasticità nel metodo di ottimizzazione (prendiamo come riferimento sempre la _steepest descent_).\n",
    "\n",
    "Definiamo quindi una funzione di loss \"_parametrica_\", i cui parametri sono una singola coppia di $\\mathcal{T}$, cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{(\\boldsymbol{x},\\boldsymbol{y})}(\\boldsymbol{w}):= |\\boldsymbol{y} - \\hat{\\boldsymbol{F}}(\\boldsymbol{x})| \\,;\n",
    "\\end{equation}\n",
    "\n",
    "Data questa loss, il metodo cambia \"spezzando\" il passo (A) dell'_Approccio 1_ in tanti sottopassi, uno per ogni coppia in $\\mathcal{T}$. Scelgo un punto casuale ogni volta e la mia funzione loss si aggiorna tenendo in considerazione l'ultimo punto valutato.\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"variabile\"; avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 2)_**:**\n",
    "\n",
    "L'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **\"Mescolo\" le coppie in** $\\mathcal{T}$;\n",
    "    2. **Per ogni** $(\\boldsymbol{x}, \\boldsymbol{y})$ in $\\mathcal{T}$ \"mescolato\":\n",
    "        1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{(\\boldsymbol{x},\\boldsymbol{y})}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è sempre definito _EPOCA DI ADDESTRAMENTO_, come per l'_Approccio 1_.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, è sempre il _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-cabinet",
   "metadata": {},
   "source": [
    "#### Approccio 3: Metodi Mini-Batch\n",
    "\n",
    "L'approccio di fatto più utilizzato nella pratica è una via di mezzo tra il primo ed il terzo (ed in realtà i primi due approcci sono casi particolari di questo).\n",
    "\n",
    "Continuiamo a definire una funzione di loss \"_parametrica_\", i cui parametri non sono una singola coppia di $\\mathcal{T}$, ma un suo arbitrario sottoinsieme $\\mathcal{B}$ (chiamato _minibatch_), cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{\\,\\mathcal{B}}(\\boldsymbol{w}):= \\sum_{(\\boldsymbol{x}, \\boldsymbol{y})\\in\\mathcal{B}}|\\boldsymbol{y} - \\hat{\\boldsymbol{F}}(\\boldsymbol{x})| \\,;\n",
    "\\end{equation}\n",
    "\n",
    "Data questa loss, il metodo cambia \"spezzando\" il passo (A) dell'_Approccio 1_ in tanti sottopassi, analogamente all'_Approccio 2_, uno per ogni sottoinsieme $\\mathcal{B}$ estratto.\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"variabile\"; avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 3)_**:**\n",
    "\n",
    "Fissato un numero $K\\in\\mathbb{N}$ di minibatch in cui \"spezzare\" $\\mathcal{T}$ (generalmente si fa secondo una cardinalità comune dei minibatch), l'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **Genero Casualmente** una $K$-partizione in minibatch _distinti_ $\\mathcal{B}_1,\\ldots , \\mathcal{B}_K$ di $\\mathcal{T}$;\n",
    "    2. **Per ogni** $k=1,\\ldots , K$:\n",
    "        1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{\\,\\mathcal{B}_k}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è sempre definito _EPOCA DI ADDESTRAMENTO_, come per gli _Approcci 1_ e _2_.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, è sempre il _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "  \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-trader",
   "metadata": {},
   "source": [
    "#### Loss Notevoli\n",
    "\n",
    "A seconda della funzione $\\boldsymbol{F}$ da apprendere, la loss su $\\mathcal{T}$, $(\\boldsymbol{x}, \\boldsymbol{y})$ o $\\mathcal{B}$ può anche essere definita in maniera differente, scegliendo un _errore elemento-per-elemento_ o un _aggregatore_ differenti. La loss sopra considerata è una _somma di errori assoluti_, ma altre equivalenti possono essere definite, per esempio:\n",
    "1. _Media di Errori Quadratici_ (_Mean Square Error_, **MSE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\frac{1}{T}\\sum_{i=1}^{T} \\left(\\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) \\right)^2$;\n",
    "2. _Media di Errori Assoluti_ (_Mean Absolute Error_, **MAE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\frac{1}{T}\\sum_{i=1}^{T} | \\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) |$;\n",
    "3. _Somma di Errori Quadratici_ (_Sum of Square Error_, **SSE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):= \\sum_{i=1}^{T} \\left(\\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) \\right)^2$;\n",
    "4. _Somma di Errori Assoluti_ (_Sum of Absolute Error_, **SAE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):= \\sum_{i=1}^{T} | \\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) |$.\n",
    "\n",
    "Le loss sopra indicate sono utili per problemi di _regressione_ e per la spiegazione della teoria. Per problemi di classificazione si utilizza generalmente altre loss (p.e. la _cross-entropy loss_) e/o particolari output layer (_softmax layers_). Non li tratteremo per concentrarci sulle proprietà generali dell'addestramento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-transformation",
   "metadata": {},
   "source": [
    "## Criteri di Arresto\n",
    "\n",
    "Come anticipato nelle scorse esercitazioni, per le NN il validation set $\\mathcal{V}$ svolge un ruolo generalmente differente rispetto ai classici algoritmi di ML.\n",
    "\n",
    "Per capire meglio, bisogna richiamare due fenomeni che possono avvenire in caso di _cattivo addestramento_ della NN:\n",
    "1. **Underfitting:** la NN si è addestrata male su $\\mathcal{T}$ ed ha quindi cattive performance sia su di esso che sul test set $\\mathcal{P}$.\n",
    "2. **Overfitting:** la NN si è addestrata \"_troppo_\" su $\\mathcal{T}$. Ha quindi imparato molto bene ad associare le $\\boldsymbol{x}$ di $\\mathcal{T}$ alle corrispondenti $\\boldsymbol{y}$, ma al prezzo di non essere capace di fare altrettanto con delle $\\boldsymbol{x}$ \"nuove\". La NN ha quindi buone performance su $\\mathcal{T}$ e cattive su $\\mathcal{P}$.\n",
    "\n",
    "Il validation set $\\mathcal{V}$, nelle NN, viene generalmente utilizzato per monitorare le ipotetiche performance del modello su $\\mathcal{P}$.\n",
    "\n",
    "I criteri di arresto principali (e più semplici) per l'addestramento di una NN sono quindi i seguenti:\n",
    "1. Fissare un numero massimo $e_{\\max}\\in\\mathbb{N}$ di epoche da eseguire;\n",
    "2. Fissare una numero massimo $p\\in\\mathbb{N}$ (pazienza) di epoche di tolleranza rispetto al quale posso accettare che la loss su $\\mathcal{V}$ cresca invece di diminuire. Oltre a ridurre i tempi di addestramento, questo criterio serve ad impedire di addestrare eccessivamente il modello e così rischiare di cadere in **overfitting**. Posso dare una p anche tanto grande, perderei solo tempo ma non precisione del metodo.\n",
    "\n",
    "---------------\n",
    "\n",
    "**OSSERVAZIONE** _(Ottimizzazione \"Classica\" e di NN)_**:**\n",
    "\n",
    "Mentre nei problemi di minimizzazione classici l'obiettivo è trovare $\\boldsymbol{x}^*$ che minimizzi globalmente (o al peggio localmente) una funzione _valutabile_, le NN sono invece tipicamente utilizzate per \"apprendere\" funzioni $\\boldsymbol{F}$ ignote o _difficilmente/onerosamente valutabili_ che caratterizzano le associazioni input-output di un numero limitato di campioni noti.\n",
    "\n",
    "La difficoltà sta quindi nel voler minimizzare l'errore sul test set (rappresentante dati \"nuovi\" ed \"ignoti\"), attraverso una procedura che sfrutta _altri_ dati (quelli del training set). Questo approccio, dettato dalla necessità, è il motivo alla base del fenomeno di overfitting e spiega la natura euristica/empirica di alcuni metodi di ottimizzazione utilizzati per addestrare le NN. Infatti, non necessariamente un minimo globale/locale per la loss su $\\mathcal{T}$ si traduce in una loss altrettanto bassa per $\\mathcal{V}$ e $\\mathcal{P}$. \n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-given",
   "metadata": {},
   "source": [
    "## MLP e Applicazioni\n",
    "\n",
    "\n",
    "Gli MLP, ed in generale le NN caratterizzate da soli FC layer, sono principalmente utilizzate per problemi di regressione/classificazione con input vettoriali non eccessivamente grandi (indicativamente $\\mathbb{R}^n$ con $n$ dell'ordine delle centinaia).\n",
    "\n",
    "Per problemi di grandi dimensioni un FC layer inizia a risultare oneroso in termini di memoria. Infatti, considerando un FC layer con stesso numero di unità $n$ del layer precedente, il numero di parametri da ottimizzare per solo questo layer è pari a $n^2 + n$.\n",
    "\n",
    "Per questo motivo gli MLP non sono mai eccessivamente profondi né hanno troppe unità per livello.\n",
    "\n",
    "Una particolare famiglia di layer utile ad aggirare queste difficoltà è quella dei _Convolutional Layers_ (utilizzati soprattutto nell'ambito della computer vision, dove la vettorializzazione di immagini significherebbe vettori con milioni di elementi).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-strain",
   "metadata": {},
   "source": [
    "# Esercitazione: MLP in Scikit-Learn\n",
    "\n",
    "\n",
    "In questa esercitazione applicheremo la classe _MLPClassifier_ di scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) al problema di classificazione di volti affrontato in precedenza con le SVM.\n",
    "\n",
    "Dato il non trascurabile numero di pixel per le immagini (per quanto a bassa risoluzione), applicheremo l'MLP a dei dati trasformati tramite PCA.\n",
    "\n",
    "**ATTENZIONE:** per dettagli sul dataset utilizzato, guardare le precedenti esercitazioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organic-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOTA BENE! *****\n",
    "# perché %matplotlib widget funzioni, installare nell'ambiente virtuale \n",
    "# il pacchetto ipympl con il comando:\n",
    "# pip install ipympl\n",
    "#\n",
    "# ATTENZIONE: perché funzioni è necessario chiudere e rilanciare jupyter-lab\n",
    "#\n",
    "# STILE DI VISUALIZZAZIONE PLOT FATTI CON MATPLOTLIB\n",
    "%matplotlib widget\n",
    "#\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV #per riaddestrare gli iperparametri dato che gli MLP sono molto sensibili a questi\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from IPython.display import display\n",
    "\n",
    "# Il codice presente di seguito serve nel caso si verifichi un errore del tipo\n",
    "#\n",
    "# \"URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1124)>\"\n",
    "#\n",
    "# al momento di chiamare la funzione fetch_lfw_people di sklearn.datasets\n",
    "#\n",
    "# ATTENZIONE: il codice di seguito non è quindi sempre necessario; se non lo fosse, commentarlo pure.\n",
    "#\n",
    "\n",
    "import os, ssl\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-bosnia",
   "metadata": {},
   "source": [
    "## Importazione del Dataset e Creazione di Training, Validation e Test set\n",
    "\n",
    "Importiamo il dataset $\\mathcal{D}$ da scikit-learn e dividiamolo in $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$. Utilizzare le seguenti percentuali:\n",
    "1. $|\\mathcal{T}| = 40\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{V}| = 10\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{P}| = 50\\% |\\mathcal{D}|$\n",
    "\n",
    "**ATTENZIONE:** visto che la classe MLPClassifier lo esegue in automatico, _NON_ sarà necessario trasformare le classi secondo la codifica del one-hot encoding.\n",
    "\n",
    "**ESERCIZIO:** completare il codice nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "enormous-jackson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_trainval</th>\n",
       "      <th>X_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N. samples</th>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N.features</th>\n",
       "      <td>1850</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            X_trainval  X_test\n",
       "N. samples         644     644\n",
       "N.features        1850    1850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "face_data = lfw_people['data']\n",
    "face_images = lfw_people['images']\n",
    "face_tnames = lfw_people['target_names']\n",
    "face_targets = lfw_people['target']\n",
    "\n",
    "# Creare gli X_trainval, y_trainval, X_test, y_test\n",
    "# (RICORDA: il validation set viene creato \"internamente\" dalla classe MLPClassifier. \n",
    "# Gli deve essere solamente specificata la percentuale rispetto al training set)\n",
    "\n",
    "random_state = 20210527\n",
    "test_p = 0.5\n",
    "val_p = 0.2 # Percentuale di dati di X_trainval da usare come validation set\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(face_data, face_targets, test_size= test_p, \n",
    "                                                          random_state= random_state, shuffle=True)\n",
    "\n",
    "display(pd.DataFrame({'X_trainval': X_trainval.shape, 'X_test': X_test.shape}, index=['N. samples', 'N.features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-ceremony",
   "metadata": {},
   "source": [
    "### PCA sul Training e Validation Set\n",
    "\n",
    "Prepariamo una PCA rispettoa i dati di *X_trainval*. Non considerimo quelli di *X_test* poiché simulato dati non a disposizione.\n",
    "\n",
    "Selezionare un numero di PC che spieghino il $95\\%$ della varianza totale dei dati.\n",
    "\n",
    "**NOTA:** A voler essere precisi, ancora meglio sarebbe stato preparare la PCA solo sui dati di training, escludendo quelli di validation. Tuttavia (per quanto mi risulta) la classe _MLPClassifier_ accetta solamente la fraione di training da usare come validation, non direttamente dei dati. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "union-outreach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero PC</th>\n",
       "      <th>% Varianza Tot. Spiegata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_trainval</th>\n",
       "      <td>123</td>\n",
       "      <td>0.950418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Numero PC  % Varianza Tot. Spiegata\n",
       "X_trainval        123                  0.950418"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparazione PCA\n",
    "\n",
    "pca = PCA(0.95)\n",
    "\n",
    "pca.fit(X_trainval)\n",
    "\n",
    "display(pd.DataFrame({'Numero PC': pca.n_components_, \n",
    "                      '% Varianza Tot. Spiegata': pca.explained_variance_ratio_.sum()}, \n",
    "                     index=['X_trainval']))\n",
    "\n",
    "# Trasformazione dati. Salvare i vecchi in \"copie di backup\"\n",
    "\n",
    "X_trainval_old = X_trainval.copy()\n",
    "X_trainval = pca.transform(X_trainval)\n",
    "\n",
    "X_test_old = X_test.copy()\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-morrison",
   "metadata": {},
   "source": [
    "### Costruzione e Addestramento MLP\n",
    "\n",
    "Costruiamo un MLP caratterizzato da 5 hidden layers con 256 unità ognuno e funzione di attivazione _relu_.\n",
    "\n",
    "Come parametri di addestramento, si utilizzino quelli qui indicati:\n",
    "1. **Batch size**: 32;\n",
    "2. **Massimo numero di epoche**: 1000;\n",
    "3. **Early stopping**: pazienza di 150 epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "small-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[265, 265, 265, 265, 265]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[265]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adverse-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione iper-parametri MLP\n",
    "hidden_layer_sizes = [256 for i in range(0,5)]\n",
    "activation = 'relu'\n",
    "patience = 75\n",
    "max_epochs = 5000\n",
    "verbose = False\n",
    "batch_sz = 4\n",
    "\n",
    "# Inizializzazione MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    activation= activation, \n",
    "                    solver = 'adam', \n",
    "                    batch_size = batch_sz,\n",
    "                    verbose=True, \n",
    "                    #learning_rate = 'invscaling',\n",
    "                    random_state = random_state, \n",
    "                    early_stopping=True, \n",
    "                    validation_fraction=val_p, \n",
    "                    max_iter=max_epochs, \n",
    "                    n_iter_no_change=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "metropolitan-darkness",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.41390402\n",
      "Validation score: 0.503876\n",
      "Iteration 2, loss = 1.61963955\n",
      "Validation score: 0.604651\n",
      "Iteration 3, loss = 0.74398403\n",
      "Validation score: 0.573643\n",
      "Iteration 4, loss = 0.33308529\n",
      "Validation score: 0.620155\n",
      "Iteration 5, loss = 0.33641665\n",
      "Validation score: 0.620155\n",
      "Iteration 6, loss = 0.41406629\n",
      "Validation score: 0.441860\n",
      "Iteration 7, loss = 0.61199823\n",
      "Validation score: 0.604651\n",
      "Iteration 8, loss = 0.37178093\n",
      "Validation score: 0.620155\n",
      "Iteration 9, loss = 0.29513462\n",
      "Validation score: 0.651163\n",
      "Iteration 10, loss = 0.60612106\n",
      "Validation score: 0.713178\n",
      "Iteration 11, loss = 0.25484818\n",
      "Validation score: 0.658915\n",
      "Iteration 12, loss = 0.19233292\n",
      "Validation score: 0.689922\n",
      "Iteration 13, loss = 0.16158572\n",
      "Validation score: 0.666667\n",
      "Iteration 14, loss = 0.39878029\n",
      "Validation score: 0.658915\n",
      "Iteration 15, loss = 0.38033873\n",
      "Validation score: 0.689922\n",
      "Iteration 16, loss = 0.11859003\n",
      "Validation score: 0.620155\n",
      "Iteration 17, loss = 0.19775884\n",
      "Validation score: 0.751938\n",
      "Iteration 18, loss = 0.39791725\n",
      "Validation score: 0.596899\n",
      "Iteration 19, loss = 0.40052092\n",
      "Validation score: 0.713178\n",
      "Iteration 20, loss = 0.10584390\n",
      "Validation score: 0.759690\n",
      "Iteration 21, loss = 0.14342971\n",
      "Validation score: 0.728682\n",
      "Iteration 22, loss = 0.21300943\n",
      "Validation score: 0.689922\n",
      "Iteration 23, loss = 0.11594613\n",
      "Validation score: 0.705426\n",
      "Iteration 24, loss = 0.12393931\n",
      "Validation score: 0.744186\n",
      "Iteration 25, loss = 0.16980900\n",
      "Validation score: 0.720930\n",
      "Iteration 26, loss = 0.13381498\n",
      "Validation score: 0.744186\n",
      "Iteration 27, loss = 0.03667044\n",
      "Validation score: 0.728682\n",
      "Iteration 28, loss = 0.01699950\n",
      "Validation score: 0.751938\n",
      "Iteration 29, loss = 0.01602415\n",
      "Validation score: 0.744186\n",
      "Iteration 30, loss = 0.01596265\n",
      "Validation score: 0.736434\n",
      "Iteration 31, loss = 0.01594236\n",
      "Validation score: 0.736434\n",
      "Iteration 32, loss = 0.01592511\n",
      "Validation score: 0.736434\n",
      "Iteration 33, loss = 0.01590916\n",
      "Validation score: 0.736434\n",
      "Iteration 34, loss = 0.01589377\n",
      "Validation score: 0.736434\n",
      "Iteration 35, loss = 0.01587860\n",
      "Validation score: 0.736434\n",
      "Iteration 36, loss = 0.01586347\n",
      "Validation score: 0.736434\n",
      "Iteration 37, loss = 0.01584755\n",
      "Validation score: 0.728682\n",
      "Iteration 38, loss = 0.01582992\n",
      "Validation score: 0.728682\n",
      "Iteration 39, loss = 0.01581231\n",
      "Validation score: 0.728682\n",
      "Iteration 40, loss = 0.01579525\n",
      "Validation score: 0.736434\n",
      "Iteration 41, loss = 0.01577782\n",
      "Validation score: 0.744186\n",
      "Iteration 42, loss = 0.01575983\n",
      "Validation score: 0.736434\n",
      "Iteration 43, loss = 0.01574105\n",
      "Validation score: 0.728682\n",
      "Iteration 44, loss = 0.01572144\n",
      "Validation score: 0.720930\n",
      "Iteration 45, loss = 0.01570101\n",
      "Validation score: 0.720930\n",
      "Iteration 46, loss = 0.01567958\n",
      "Validation score: 0.720930\n",
      "Iteration 47, loss = 0.01565715\n",
      "Validation score: 0.720930\n",
      "Iteration 48, loss = 0.01563364\n",
      "Validation score: 0.720930\n",
      "Iteration 49, loss = 0.01560895\n",
      "Validation score: 0.720930\n",
      "Iteration 50, loss = 0.01558307\n",
      "Validation score: 0.720930\n",
      "Iteration 51, loss = 0.01555597\n",
      "Validation score: 0.713178\n",
      "Iteration 52, loss = 0.01552745\n",
      "Validation score: 0.713178\n",
      "Iteration 53, loss = 0.01549750\n",
      "Validation score: 0.713178\n",
      "Iteration 54, loss = 0.01546607\n",
      "Validation score: 0.713178\n",
      "Iteration 55, loss = 0.01543303\n",
      "Validation score: 0.713178\n",
      "Iteration 56, loss = 0.01539834\n",
      "Validation score: 0.720930\n",
      "Iteration 57, loss = 0.01536184\n",
      "Validation score: 0.713178\n",
      "Iteration 58, loss = 0.01532348\n",
      "Validation score: 0.720930\n",
      "Iteration 59, loss = 0.01528323\n",
      "Validation score: 0.713178\n",
      "Iteration 60, loss = 0.01524090\n",
      "Validation score: 0.720930\n",
      "Iteration 61, loss = 0.01519640\n",
      "Validation score: 0.720930\n",
      "Iteration 62, loss = 0.01514969\n",
      "Validation score: 0.720930\n",
      "Iteration 63, loss = 0.01510060\n",
      "Validation score: 0.720930\n",
      "Iteration 64, loss = 0.01504915\n",
      "Validation score: 0.720930\n",
      "Iteration 65, loss = 0.01499503\n",
      "Validation score: 0.720930\n",
      "Iteration 66, loss = 0.01493829\n",
      "Validation score: 0.720930\n",
      "Iteration 67, loss = 0.01487873\n",
      "Validation score: 0.720930\n",
      "Iteration 68, loss = 0.01481623\n",
      "Validation score: 0.720930\n",
      "Iteration 69, loss = 0.01475082\n",
      "Validation score: 0.720930\n",
      "Iteration 70, loss = 0.01468217\n",
      "Validation score: 0.720930\n",
      "Iteration 71, loss = 0.01461026\n",
      "Validation score: 0.720930\n",
      "Iteration 72, loss = 0.01453500\n",
      "Validation score: 0.720930\n",
      "Iteration 73, loss = 0.01445626\n",
      "Validation score: 0.720930\n",
      "Iteration 74, loss = 0.01437367\n",
      "Validation score: 0.720930\n",
      "Iteration 75, loss = 0.01428742\n",
      "Validation score: 0.720930\n",
      "Iteration 76, loss = 0.01419711\n",
      "Validation score: 0.728682\n",
      "Iteration 77, loss = 0.01410306\n",
      "Validation score: 0.728682\n",
      "Iteration 78, loss = 0.01400449\n",
      "Validation score: 0.728682\n",
      "Iteration 79, loss = 0.01390177\n",
      "Validation score: 0.728682\n",
      "Iteration 80, loss = 0.01379456\n",
      "Validation score: 0.736434\n",
      "Iteration 81, loss = 0.01368270\n",
      "Validation score: 0.728682\n",
      "Iteration 82, loss = 0.01356616\n",
      "Validation score: 0.728682\n",
      "Iteration 83, loss = 0.01344488\n",
      "Validation score: 0.744186\n",
      "Iteration 84, loss = 0.01331845\n",
      "Validation score: 0.728682\n",
      "Iteration 85, loss = 0.01318707\n",
      "Validation score: 0.736434\n",
      "Iteration 86, loss = 0.01305048\n",
      "Validation score: 0.744186\n",
      "Iteration 87, loss = 0.01290852\n",
      "Validation score: 0.728682\n",
      "Iteration 88, loss = 0.01276125\n",
      "Validation score: 0.736434\n",
      "Iteration 89, loss = 0.01260838\n",
      "Validation score: 0.736434\n",
      "Iteration 90, loss = 0.01245002\n",
      "Validation score: 0.736434\n",
      "Iteration 91, loss = 0.01228590\n",
      "Validation score: 0.736434\n",
      "Iteration 92, loss = 0.01211609\n",
      "Validation score: 0.736434\n",
      "Iteration 93, loss = 0.01194058\n",
      "Validation score: 0.736434\n",
      "Iteration 94, loss = 0.01175915\n",
      "Validation score: 0.736434\n",
      "Iteration 95, loss = 0.01157197\n",
      "Validation score: 0.736434\n",
      "Iteration 96, loss = 0.01137886\n",
      "Validation score: 0.744186\n",
      "Validation score did not improve more than tol=0.000100 for 75 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=4, early_stopping=True,\n",
       "              hidden_layer_sizes=[256, 256, 256, 256, 256], max_iter=5000,\n",
       "              n_iter_no_change=75, random_state=20210527,\n",
       "              validation_fraction=0.2, verbose=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addestramento MLP\n",
    "mlp.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cleared-kuwait",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train. + val.</th>\n",
       "      <td>0.944099</td>\n",
       "      <td>0.946179</td>\n",
       "      <td>0.944099</td>\n",
       "      <td>0.944381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.710559</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.691768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision    Recall        F1\n",
       "train. + val.  0.944099   0.946179  0.944099  0.944381\n",
       "test           0.695652   0.710559  0.695652  0.691768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>217</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon                 20             8                3              5   \n",
       "Colin Powell                  2            86                0             11   \n",
       "Donald Rumsfeld               5             8               34             11   \n",
       "George W Bush                 0             2                5            217   \n",
       "Gerhard Schroeder             1             1                0              9   \n",
       "Hugo Chavez                   0             0                0             16   \n",
       "Tony Blair                    0             3                1              6   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                       1            1           5  \n",
       "Colin Powell                       3            4           5  \n",
       "Donald Rumsfeld                    3            3           4  \n",
       "George W Bush                      5            2          29  \n",
       "Gerhard Schroeder                 22            2          15  \n",
       "Hugo Chavez                        3           14           8  \n",
       "Tony Blair                         3            3          55  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.116279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.045045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.834615</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.111538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.195122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.774648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.465116      0.186047         0.069767       0.116279   \n",
       "Colin Powell           0.018018      0.774775         0.000000       0.099099   \n",
       "Donald Rumsfeld        0.073529      0.117647         0.500000       0.161765   \n",
       "George W Bush          0.000000      0.007692         0.019231       0.834615   \n",
       "Gerhard Schroeder      0.020000      0.020000         0.000000       0.180000   \n",
       "Hugo Chavez            0.000000      0.000000         0.000000       0.390244   \n",
       "Tony Blair             0.000000      0.042254         0.014085       0.084507   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.023256     0.023256    0.116279  \n",
       "Colin Powell                0.027027     0.036036    0.045045  \n",
       "Donald Rumsfeld             0.044118     0.044118    0.058824  \n",
       "George W Bush               0.019231     0.007692    0.111538  \n",
       "Gerhard Schroeder           0.440000     0.040000    0.300000  \n",
       "Hugo Chavez                 0.073171     0.341463    0.195122  \n",
       "Tony Blair                  0.042254     0.042254    0.774648  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.041322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.041322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.033058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.789091</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.239669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.123967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058182</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.066116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.021818</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.714286      0.074074         0.069767       0.018182   \n",
       "Colin Powell           0.071429      0.796296         0.000000       0.040000   \n",
       "Donald Rumsfeld        0.178571      0.074074         0.790698       0.040000   \n",
       "George W Bush          0.000000      0.018519         0.116279       0.789091   \n",
       "Gerhard Schroeder      0.035714      0.009259         0.000000       0.032727   \n",
       "Hugo Chavez            0.000000      0.000000         0.000000       0.058182   \n",
       "Tony Blair             0.000000      0.027778         0.023256       0.021818   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                   0.025     0.034483    0.041322  \n",
       "Colin Powell                   0.075     0.137931    0.041322  \n",
       "Donald Rumsfeld                0.075     0.103448    0.033058  \n",
       "George W Bush                  0.125     0.068966    0.239669  \n",
       "Gerhard Schroeder              0.550     0.068966    0.123967  \n",
       "Hugo Chavez                    0.075     0.482759    0.066116  \n",
       "Tony Blair                     0.075     0.103448    0.454545  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance\n",
    "\n",
    "y_pred_trainval = mlp.predict(X_trainval)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "acc_trainval = mlp.score(X_trainval, y_trainval)\n",
    "prec_trainval = precision_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "rec_trainval = recall_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "f1_trainval = f1_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "\n",
    "acc = mlp.score(X_test, y_test)\n",
    "prec = precision_score(y_test, y_pred, average='weighted')\n",
    "rec = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "df_perf = pd.DataFrame({'Accuracy': [acc_trainval, acc], \n",
    "                        'Precision': [prec_trainval, prec], \n",
    "                        'Recall': [rec_trainval, rec],\n",
    "                        'F1': [f1_trainval, f1]\n",
    "                       },\n",
    "                      index=['train. + val.', 'test'])\n",
    "\n",
    "cmat = confusion_matrix(y_test, y_pred, labels=mlp.classes_)\n",
    "cmat_norm_true = confusion_matrix(y_test, y_pred, labels=mlp.classes_, normalize='true')\n",
    "cmat_norm_pred = confusion_matrix(y_test, y_pred, labels=mlp.classes_, normalize='pred')\n",
    "\n",
    "df_cmat = pd.DataFrame(cmat, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_true = pd.DataFrame(cmat_norm_true, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_pred = pd.DataFrame(cmat_norm_pred, columns=face_tnames, index=face_tnames)\n",
    "\n",
    "display(df_perf)\n",
    "display(df_cmat)\n",
    "display(df_cmat_norm_true)\n",
    "display(df_cmat_norm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-arcade",
   "metadata": {},
   "source": [
    "## Alcuni Esempi Visivi\n",
    "\n",
    "Mostriamo visivamente come viene fatta la classificazione multi-classe. \n",
    "\n",
    "**RICORDA:** la classe MLPClassifier ha un metodo _predict_proba_ che restituisce le probabilità di appartenenza di un input ad ognuna delle classi. Quella con probabilità massima è poi quella predetta in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "spectacular-birth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7e90de769d47bfa98364ee0d02e867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bc61af6a92429d8cadeee7409f6ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b829d700bd43f699e9ade008b1d57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e08e8da0bf4e749b0637bdcbcb1a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c3a301607b4a29be448cfbd21a1b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14233431c1b0415b93be297d9c034299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831472cc72ac4d54b87d3321ca511d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f24a128d8c34c37a768d904b2045639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfb983e43ff48c89bdb4ea2e3876ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde9676fc2984454afa8ead923d2ad69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414563e69e91498f81b437240357d91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d665b6b9614a30924142bde06be932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d151bd0abd4017b237076bf5d0e96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5635bde46494eb38054cfdb2e45e5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40181b96a9e415f8de51beed17c9118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a86bdf263b4ba499d2ad496d3848ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870f13fce97b4511ba300e77f3e4d56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7503534243dc4b73b6e81f70eae6bacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3401ccb5694d45a65d339ca03615e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33f8ef648fa423d9d05547670320385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b82ab7303a8442792599d5c9e4f3ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406892cfdc8447038b3881431b7b29d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9960017162d944ea8980f8add3fc1964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63bf7704b114227a1b09b95a5adcf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02e5ffbbd094096b8550635ab90a683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Abbreviazione nomi per etichette in barplot\n",
    "face_tnames_short = []\n",
    "for name in face_tnames:\n",
    "    name_split = name.split(' ')\n",
    "    nm = ''\n",
    "    for word in name_split:\n",
    "        nm = nm + word[0]\n",
    "    face_tnames_short.append(nm)\n",
    "\n",
    "# Selezione di \"n_randsamples\" volti random dal dataset\n",
    "\n",
    "n_randsamples = 25\n",
    "ind_test_rand = np.random.choice(len(y_test), n_randsamples, replace=False)\n",
    "\n",
    "# Matrice delle n_randsamples volti scelti (una riga, un volto)\n",
    "rand_faces = X_test_old[ind_test_rand, :]\n",
    "rand_targets = y_test[ind_test_rand]\n",
    "\n",
    "# Decision Function per i volti random:\n",
    "rand_faces_decision = mlp.predict_proba(pca.transform(rand_faces))\n",
    "y_pred_rand_faces = mlp.predict(pca.transform(rand_faces))\n",
    "\n",
    "for i in range(n_randsamples):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ii = ind_test_rand[i]\n",
    "    face_ii = face_images[ii]\n",
    "    \n",
    "    axs[0].imshow(face_ii, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Volto {} ({})'.format(ii, face_tnames[face_targets[ii]]))\n",
    "    \n",
    "    axs[1].bar(np.arange(len(face_tnames)),\n",
    "               rand_faces_decision[i, :]\n",
    "              )\n",
    "    axs[1].grid()\n",
    "    axs[1].set_xticks(np.arange(len(face_tnames)))\n",
    "    axs[1].set_xticklabels(face_tnames_short,\n",
    "                           rotation=15,\n",
    "                           fontsize=12\n",
    "                          )\n",
    "    axs[1].set_title('Predizione: {}'.format(face_tnames[y_pred_rand_faces[i]]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dietary-burst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AS', 'CP', 'DR', 'GWB', 'GS', 'HC', 'TB']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_tnames_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-hearts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-fountain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
