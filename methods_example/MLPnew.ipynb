{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passive-pathology",
   "metadata": {},
   "source": [
    "# Perceptroni Multi-Strato\n",
    "\n",
    "I _Perceptroni Multi-Strato_ (_Multi-Layer Perceptron_, **MLP**) sono l'esempio tipico di _Rete Neurale_ (_Neural Network_:, **NN**).\n",
    "\n",
    "## MLP in Breve\n",
    "\n",
    "Gli MLP sono un particolare tipo di _feedforward_ NN (cioè senza ricorsività/cicli nella sua struttura) caratterizzato da una sequenza di _strati completamente connessi_ (_fully-connected layers_, *FC Layers*).\n",
    "\n",
    "------------------------------\n",
    "**\"DEFINIZIONE\"** _(Funzione caratterizzante un Fully-Connected Layer)_**:**\n",
    "\n",
    "Sia $L$ un FC layer di $m\\in\\mathbb{N}$ unità con funzione di attivazione $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ e sia $L$ (completamente) connesso con un altro livello di $n$ unità. Il layer $L$ è quindi caratterizzato dalla funzione $\\mathcal{L}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\boldsymbol{x}) := \\boldsymbol{\\sigma}\\left(W\\boldsymbol{x} + \\boldsymbol{b}\\right)\\,,\\quad \\forall \\ \\boldsymbol{x}\\in\\mathbb{R}^n\\,,\n",
    "\\end{equation}\n",
    "dove:\n",
    "- $W\\in\\mathbb{R}^{m\\times n}$ è la matrice dei pesi del livello $L$;\n",
    "- $\\boldsymbol{b}\\in\\mathbb{R}^m$ è il vettore dei bias\n",
    "- $\\boldsymbol{\\sigma}:\\mathbb{R}^m\\rightarrow\\mathbb{R}^m$ è una funzione vettoriale che applica elemento-per-elemento la funzione $\\sigma$.\n",
    "------------------------------\n",
    "\n",
    "------------------------------\n",
    "**\"DEFINIZIONE\"** _(Input Layer)_**:**\n",
    "\n",
    "Un _Input Layer_ di $n\\in\\mathbb{N}$ unità è un layer che \"_legge_\" vettori di $\\mathbb{R}^n$ e li \"_invia_\" ai layer successivi con lui connessi.\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-hudson",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "**\"DEFINIZIONE\"** _(Funzione caratterizzante un Multi-Layer Perceptron)_**:**\n",
    "\n",
    "Sia dato un MLP costituito da un input layer $L^{(0)}$ di $n\\in\\mathbb{N}$ unità, seguito da una sequenza di FC layers $L^{(1)},\\ldots ,L^{(H)}, L^{(H+1)}$ connessi uno dopo l'altro. In particolare, i layer $L^{(1)},\\ldots ,L^{(H)}$ sono definiti _strati nascosti_ (_hidden layers_) mentre $L^{(H+1)}$ è definito _strato di output_ (_output layer_).\n",
    "\n",
    "L'MLP in questione è quindi rappresentato da una funzione $\\hat{\\boldsymbol{F}}:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\boldsymbol{x}) : \\mathbb{R}^n \\xrightarrow[]{\\mathcal{L}^{(1)}} \\mathbb{R}^{n_1} \\xrightarrow[]{\\mathcal{L}^{(2)}} \\cdots \\xrightarrow[]{\\mathcal{L}^{(H)}} \\mathbb{R}^{n_H}\\xrightarrow[]{\\mathcal{L}^{(H+1)}}\\mathbb{R}^m\n",
    "\\end{equation}\n",
    "\n",
    "e in particolare \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\boldsymbol{x}) = \\boldsymbol{\\sigma}^{(H+1)}\\left( W^{(H+1)}\\boldsymbol{\\sigma}^{(H)}\\left(\\cdots \\left( W^{(2)}\\boldsymbol{\\sigma}^{(1)}\\left(W^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}\\right)+\\boldsymbol{b}^2\\right) \\cdots\\right)  + \\boldsymbol{b}^{(H+1)}\\right)\\,,\\quad \\forall \\ \\boldsymbol{x}\\in\\mathbb{R}^n\\,,\n",
    "\\end{equation}\n",
    "\n",
    "dove:\n",
    "- $W^{(h)}$, $\\boldsymbol{b}^{(h)}$, $\\sigma^{(h)}$, sono rispettivamente i pesi, i bias e la funzione di attivazione del layer $h$-esimo, per ogni $h=1,\\ldots , H+1$;\n",
    "- $n_h\\in\\mathbb{N}$ è il numero di unità del layer $h$-esimo, per ogni $h=1,\\ldots ,H$;\n",
    "- $m\\in\\mathbb{N}$ è il numero di unità dell'output layer $L^{(H+1)}$.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "-----------------\n",
    "\n",
    "**OSSERVAZIONE** _($\\hat{\\boldsymbol{F}}$ come Funzione Parametrica)_**:** \n",
    "\n",
    "La funzione $\\hat{\\boldsymbol{F}}$ caratterizzante un MLP è, come le funzioni di tutti gli altri algoritmi di Machine Learning (ML) una funzione parametrica con parametri i pesi $W^{(h)}$ ed i bias $\\boldsymbol{b}^{(h)}$. Indicando con $\\boldsymbol{w}$ il vettore ottenuto dalla concatenazione delle vettorializzazioni di tutti i pesi $W^{(h)}$ e di tutti i bias $\\boldsymbol{b}^{(h)}$, possiamo esplicitare la dipendenza dell'MLP da questi parametri con la seguente notazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{F}}(\\ \\cdot \\ \\,;\\, \\boldsymbol{w})\\quad \\text{oppure} \\quad \\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}(\\ \\cdot \\ )\n",
    "\\end{equation}\n",
    "\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-kenya",
   "metadata": {},
   "source": [
    "## Metodi di Discesa del Gradiente\n",
    "\n",
    "\n",
    "I metodi di discesa del gradiente sono _metodi numerici iterativi_ per la minimizzazione (massimizzazione) di funzioni $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. \n",
    "\n",
    "Le funzioni da minimizzare sono generalmente definite indifferentemente come \"_funzioni obiettivo_\" (_objective functions_), \"_funzioni di costo_\" (_cost functions_) o \"_funzioni di perdita_\" (_loss functions_).\n",
    "\n",
    "Questi metodi si basano sull'osservazione che $-\\nabla f(\\boldsymbol{x}_0)$ è la _direzione di più ripida discesa_ per $f$ nel punto $\\boldsymbol{x}_0$ (analogamente, $\\nabla f(\\boldsymbol{x}_0)$ è la direzione di più ripida ascesa).\n",
    "\n",
    "------------------\n",
    "\n",
    "**DEFINIZIONE** _(Direzione di Discesa)_**:**\n",
    "\n",
    "Un vettore $\\boldsymbol{p}\\in\\mathbb{R}^n$ è una _direzione di discesa_ per la funzione $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ in $\\boldsymbol{x}_0\\in\\mathbb{R}^n$ se esiste $\\alpha^*\\in\\mathbb{R}^+$ tale che\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\boldsymbol{x}_0)\\geq f(\\boldsymbol{x}_0 + \\alpha\\boldsymbol{p})\\,,\\quad \\forall \\ \\alpha\\in [0, \\alpha^*]\n",
    "\\end{equation}\n",
    "\n",
    "------------------\n",
    "\n",
    "------------------\n",
    "\n",
    "**METODO** _(Steepest Descent - a grandi linee...)_**:**\n",
    "\n",
    "Il _metodo di più ripida discesa_ (_steepest descent method_) è un metodo di discesa del gradiente per la minimizzazione di funzioni $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$, cioè per trovare la soluzione al problema\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\boldsymbol{x}\\in\\mathbb{R}^n} f(\\boldsymbol{x})\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Dato quindi un punto di partenza _arbitrario_ $\\boldsymbol{x}_0\\in\\mathbb{R}^n$, abbiamo che il passo $k$-esimo del metodo è definito dalla seguente operazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_{k} - \\alpha_{k}\\nabla f(\\boldsymbol{x}_k)\\,, \\quad \\forall \\ k\\geq 0\\,,\n",
    "\\end{equation}\n",
    "\n",
    "con $\\alpha_k\\in\\mathbb{R}^+$ è un _fattore di moltiplicazione del passo di discesa_ (nei casi più semplici, $\\alpha_k=\\alpha$ costante per ogni $k$).\n",
    "\n",
    "\n",
    "------------------\n",
    "\n",
    "------------------\n",
    "\n",
    "**LAVORARE CON IL METODO DI PIU' RIPIDA DISCESA** _(Proprietà e Problemi)_**:**\n",
    "\n",
    "1. Sotto specifiche ipotesi di regolarità per $f$ e/o opportune scelte di $\\boldsymbol{x}_0$ ed $\\{\\alpha_k\\}_{k\\in\\mathbb{N}}$, il metodo _CONVERGE_ ad un minimo _LOCALE_ della funzione;\n",
    "\n",
    "2. Se non si hanno informazioni sulle proprietà di $f$, la convergenza del metodo è altamente influenzata dalla scelta del punto di partenza $\\boldsymbol{x}_0$ e dalla successione $\\{\\alpha_k\\}_{k\\in\\mathbb{N}}$ di moltiplicatori del passo. Il metodo potrebbe quindi CONVERGERE, DIVERGERE, \"OSCILLLARE\";\n",
    "\n",
    "3. Vanno stabiliti opportuni criteri di arresto (altrimenti il metodo contrinuerebbe all'infinito). Anche i parametri caratterizzanti i criteri di arresto influenzano la convergenza del metodo.\n",
    "\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-small",
   "metadata": {},
   "source": [
    "\n",
    "## Addestramento di un MLP - in Breve\n",
    "\n",
    "L'addestramento di un MLP (e di una NN in generale) richiede sia conoscenze teoriche dei metodi di ottimizzazione numerica che una certa sensibilità pratica/empirica per la scelta dei parametri che li caratterizzano. \n",
    "\n",
    "------------------\n",
    "**ATTENZIONE** _(Aggiunta di una Componente Stocastica)_**:** \n",
    "\n",
    "Si deve inoltre considerare che l'addestramento di NN introduce delle componenti _stocastiche_ nei metodi numerici sopra citati. In altre parole, non si minimizza una generica funzione di costo \"_fissata_\", ma una funzione di costo caratterizzata dai campioni del mio dataset che uso per l'addestramento.\n",
    "\n",
    "------------------\n",
    "\n",
    "### Funzioni di Loss Rispetto a \"Gruppi\" di Dati\n",
    "\n",
    "\n",
    "Data una funzione target $\\boldsymbol{F}$, per addestrare un MLP con funzione parametrica $\\hat{\\boldsymbol{F}}_\\boldsymbol{w}$ ad apprendere $\\boldsymbol{F}$, vorremmo _idealmente_ risolvere il problema\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\boldsymbol{w}} \\left\\lbrace \\mathrm{Loss}(\\boldsymbol{w}):= \\sum_{\\boldsymbol{x}\\in\\mathbb{R}^n}|\\boldsymbol{F}(\\boldsymbol{x}) - \\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}(\\boldsymbol{x})|\\right\\rbrace\\,;\n",
    "\\end{equation}\n",
    "in altre parole, vorremmo trovare $\\boldsymbol{w}^*$ tale che $\\mathrm{Loss}(\\boldsymbol{w}^*)=0$.\n",
    "\n",
    "**ATTENZIONE:** NOTARE CHE NEL PROBLEMA DI MINIMO LE VARIABILI SONO I PARAMETRI $\\boldsymbol{w}$, NON LE $\\boldsymbol{x}$!!\n",
    "\n",
    "Ovviamente, non disponendo degli infiniti punti $\\boldsymbol{x}\\in\\mathbb{R}^n$ (e spesso neanche della funzione target $\\boldsymbol{F}$, ma solo delle valutazioni $\\boldsymbol{y}=\\boldsymbol{F}(\\boldsymbol{x})$), il problema ideale sopra indicato va ri-adattato alla pratica, cioè ai dati disponibili contenuti nel training set\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{T}=\\{ (\\boldsymbol{x}_1,\\boldsymbol{y}_1),\\ldots ,  (\\boldsymbol{x}_T,\\boldsymbol{y}_T) \\}\\,.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-packing",
   "metadata": {},
   "source": [
    "#### Approccio 1: Niente Stocasticità\n",
    "\n",
    "L'idea più semplice per addestrare un MLP può essere quindi quello di definire una funzione di loss \"troncando\" quella del problema ideale, cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\sum_{i=1}^T \\underbrace{|\\boldsymbol{y}_i - \\hat{\\boldsymbol{F}}(\\boldsymbol{x}_i)| }_{\\text{err. el.-per-el.}} \\,.\n",
    "\\end{equation}\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"fissata\" e può essere risolto tramite il metodo di più ripida discesa (o sue varianti); avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 1)_**:**\n",
    "\n",
    "L'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è definito _EPOCA DI ADDESTRAMENTO_, nella terminologia delle NN. Il più semplice dei criteri di arresto è quindi quello di inserire un numero massimo di epoche da eseguire.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, nella terminologia delle NN, è definito _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "  \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-robinson",
   "metadata": {},
   "source": [
    "#### Approccio 2: Stocasticità \"Pura\"\n",
    "\n",
    "Poiché addestrare l'MLP rispetto ad un errore \"aggregato\" rispetto a tutto il training set potrebbe dare risultati poco sensibili ai dettagli della funzione target $\\boldsymbol{F}$, l'idea è quella di introdurre della stocasticità nel metodo di ottimizzazione (prendiamo come riferimento sempre la _steepest descent_).\n",
    "\n",
    "Definiamo quindi una funzione di loss \"_parametrica_\", i cui parametri sono una singola coppia di $\\mathcal{T}$, cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{(\\boldsymbol{x},\\boldsymbol{y})}(\\boldsymbol{w}):= |\\boldsymbol{y} - \\hat{\\boldsymbol{F}}(\\boldsymbol{x})| \\,;\n",
    "\\end{equation}\n",
    "\n",
    "Data questa loss, il metodo cambia \"spezzando\" il passo (A) dell'_Approccio 1_ in tanti sottopassi, uno per ogni coppia in $\\mathcal{T}$\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"variabile\"; avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 2)_**:**\n",
    "\n",
    "L'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **\"Mescolo\" le coppie in** $\\mathcal{T}$;\n",
    "    2. **Per ogni** $(\\boldsymbol{x}, \\boldsymbol{y})$ in $\\mathcal{T}$ \"mescolato\":\n",
    "        1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{(\\boldsymbol{x},\\boldsymbol{y})}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è sempre definito _EPOCA DI ADDESTRAMENTO_, come per l'_Approccio 1_.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, è sempre il _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-cardiff",
   "metadata": {},
   "source": [
    "#### Approccio 3: Metodi Mini-Batch\n",
    "\n",
    "L'approccio di fatto più utilizzato nella pratica è una via di mezzo tra il primo ed il terzo (ed in realtà i primi due approcci sono casi particolari di questo).\n",
    "\n",
    "Continuiamo a definire una funzione di loss \"_parametrica_\", i cui parametri non sono una singola coppia di $\\mathcal{T}$, ma un suo arbitrario sottoinsieme $\\mathcal{B}$ (chiamato _minibatch_), cioè:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Loss}_{\\,\\mathcal{B}}(\\boldsymbol{w}):= \\sum_{(\\boldsymbol{x}, \\boldsymbol{y})\\in\\mathcal{B}}|\\boldsymbol{y} - \\hat{\\boldsymbol{F}}(\\boldsymbol{x})| \\,;\n",
    "\\end{equation}\n",
    "\n",
    "Data questa loss, il metodo cambia \"spezzando\" il passo (A) dell'_Approccio 1_ in tanti sottopassi, analogamente all'_Approccio 2_, uno per ogni sottoinsieme $\\mathcal{B}$ estratto.\n",
    "\n",
    "In questo caso, il problema di minimizzazione vede una loss \"variabile\"; avremo quindi che l'addestramento del mio MLP sarà così definito:\n",
    "\n",
    "-------------------\n",
    "**SCHEMA** _(Addestramento - Approccio 3)_**:**\n",
    "\n",
    "Fissato un numero $K\\in\\mathbb{N}$ di minibatch in cui \"spezzare\" $\\mathcal{T}$ (generalmente si fa secondo una cardinalità comune dei minibatch), l'algoritmo di addestramento per questo approccio è esemplificato dai seguenti passi:\n",
    "\n",
    "1. **Inizializzazione Pesi:** genero dei pesi $\\boldsymbol{w}$ e li assegno all'architettura del mio MLP (ottengo quindi $\\hat{\\boldsymbol{F}}_{\\boldsymbol{w}}$);\n",
    "2. **Finché** non viene raggiunto un criterio di arresto (vedremo in seguito), ripetere:\n",
    "    1. **Genero Casualmente** una $K$-partizione in minibatch _distinti_ $\\mathcal{B}_1,\\ldots , \\mathcal{B}_K$ di $\\mathcal{T}$;\n",
    "    2. **Per ogni** $k=1,\\ldots , K$:\n",
    "        1. **Aggiornamento Pesi:** $\\boldsymbol{w}\\gets \\boldsymbol{w} - \\alpha \\nabla \\mathrm{Loss}_{\\,\\mathcal{B}_k}(\\boldsymbol{w})$;\n",
    "    \n",
    "**Definizione:** ogni ripetizione dei passi del punto (2) è sempre definito _EPOCA DI ADDESTRAMENTO_, come per gli _Approcci 1_ e _2_.\n",
    "\n",
    "**Definizione:** il parametro $\\alpha$, è sempre il _tasso di apprendimento_ (_learning rate_). Anche se non esplicito nella notazione utilizzata sopra, $\\alpha$ può variare da epoca ad epoca. \n",
    "  \n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-light",
   "metadata": {},
   "source": [
    "#### Loss Notevoli\n",
    "\n",
    "A seconda della funzione $\\boldsymbol{F}$ da apprendere, la loss su $\\mathcal{T}$, $(\\boldsymbol{x}, \\boldsymbol{y})$ o $\\mathcal{B}$ può anche essere definita in maniera differente, scegliendo un _errore elemento-per-elemento_ o un _aggregatore_ differenti. La loss sopra considerata è una _somma di errori assoluti_, ma altre equivalenti possono essere definite, per esempio:\n",
    "1. _Media di Errori Quadratici_ (_Mean Square Error_, **MSE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\frac{1}{T}\\sum_{i=1}^{T} \\left(\\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) \\right)^2$;\n",
    "2. _Media di Errori Assoluti_ (_Mean Absolute Error_, **MAE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):=\\frac{1}{T}\\sum_{i=1}^{T} | \\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) |$;\n",
    "3. _Somma di Errori Quadratici_ (_Sum of Square Error_, **MSE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):= \\sum_{i=1}^{T} \\left(\\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) \\right)^2$;\n",
    "4. _Somma di Errori Assoluti_ (_Sum of Absolute Error_, **MAE**): $\\mathrm{Loss}_{\\,\\mathcal{T}}(\\boldsymbol{w}):= \\sum_{i=1}^{T} | \\boldsymbol{y}_i - \\hat{F}_{\\boldsymbol{w}}(\\boldsymbol{x}_i) |$.\n",
    "\n",
    "Le loss sopra indicate sono utili per problemi di _regressione_ e per la spiegazione della teoria. Per problemi di classificazione si utilizza generalmente altre loss (p.e. la _cross-entropy loss_) e/o particolari output layer (_softmax layers_). Non li tratteremo per concentrarci sulle proprietà generali dell'addestramento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-train",
   "metadata": {},
   "source": [
    "## Criteri di Arresto\n",
    "\n",
    "Come anticipato nelle scorse esercitazioni, per le NN il validation set $\\mathcal{V}$ svolge un ruolo generalmente differente rispetto ai classici algoritmi di ML.\n",
    "\n",
    "Per capire meglio, bisogna richiamare due fenomeni che possono avvenire in caso di _cattivo addestramento_ della NN:\n",
    "1. **Underfitting:** la NN si è addestrata male su $\\mathcal{T}$ ed ha quindi cattive performance sia su di esso che sul test set $\\mathcal{P}$.\n",
    "2. **Overfitting:** la NN si è addestrata \"_troppo_\" su $\\mathcal{T}$. Ha quindi imparato molto bene ad associare le $\\boldsymbol{x}$ di $\\mathcal{T}$ alle corrispondenti $\\boldsymbol{y}$, ma al prezzo di non essere capace di fare altrettanto con delle $\\boldsymbol{x}$ \"nuove\". La NN ha quindi buone performance su $\\mathcal{T}$ e cattive su $\\mathcal{P}$.\n",
    "\n",
    "Il validation set $\\mathcal{V}$, nelle NN, viene generalmente utilizzato per monitorare le ipotetiche performance del modello su $\\mathcal{P}$.\n",
    "\n",
    "I criteri di arresto principali (e più semplici) per l'addestramento di una NN sono quindi i seguenti:\n",
    "1. Fissare un numero massimo $e_{\\max}\\in\\mathbb{N}$ di epoche da eseguire;\n",
    "2. Fissare una numero massimo $p\\in\\mathbb{N}$ di epoche di tolleranza rispetto al quale posso accettare che la loss su $\\mathcal{V}$ cresca invece di diminuire. Oltre a ridurre i tempi di addestramento, questo criterio serve ad impedire di addestrare eccessivamente il modello e così rischiare di cadere in **overfitting**.\n",
    "\n",
    "---------------\n",
    "\n",
    "**OSSERVAZIONE** _(Ottimizzazione \"Classica\" e di NN)_**:**\n",
    "\n",
    "Mentre nei problemi di minimizzazione classici l'obiettivo è trovare $\\boldsymbol{x}^*$ che minimizzi globalmente (o al peggio localmente) una funzione _valutabile_, le NN sono invece tipicamente utilizzate per \"apprendere\" funzioni $\\boldsymbol{F}$ ignote o _difficilmente/onerosamente valutabili_ che caratterizzano le associazioni input-output di un numero limitato di campioni noti.\n",
    "\n",
    "La difficoltà sta quindi nel voler minimizzare l'errore sul test set (rappresentante dati \"nuovi\" ed \"ignoti\"), attraverso una procedura che sfrutta _altri_ dati (quelli del training set). Questo approccio, dettato dalla necessità, è il motivo alla base del fenomeno di overfitting e spiega la natura euristica/empirica di alcuni metodi di ottimizzazione utilizzati per addestrare le NN. Infatti, non necessariamente un minimo globale/locale per la loss su $\\mathcal{T}$ si traduce in una loss altrettanto bassa per $\\mathcal{V}$ e $\\mathcal{P}$. \n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-portugal",
   "metadata": {},
   "source": [
    "## MLP e Applicazioni\n",
    "\n",
    "\n",
    "Gli MLP, ed in generale le NN caratterizzate da soli FC layer, sono principalmente utilizzate per problemi di regressione/classificazione con input vettoriali non eccessivamente grandi (indicativamente $\\mathbb{R}^n$ con $n$ dell'ordine delle centinaia).\n",
    "\n",
    "Per problemi di grandi dimensioni un FC layer inizia a risultare oneroso in termini di memoria. Infatti, considerando un FC layer con stesso numero di unità $n$ del layer precedente, il numero di parametri da ottimizzare per solo questo layer è pari a $n^2 + n$.\n",
    "\n",
    "Per questo motivo gli MLP non sono mai eccessivamente profondi né hanno troppe unità per livello.\n",
    "\n",
    "Una particolare famiglia di layer utile ad aggirare queste difficoltà è quella dei _Convolutional Layers_ (utilizzati soprattutto nell'ambito della computer vision, dove la vettorializzazione di immagini significherebbe vettori con milioni di elementi).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-competition",
   "metadata": {},
   "source": [
    "# Esercitazione: MLP in Scikit-Learn\n",
    "\n",
    "\n",
    "In questa esercitazione applicheremo la classe _MLPClassifier_ di scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) al problema di classificazione di volti affrontato in precedenza con le SVM.\n",
    "\n",
    "Dato il non trascurabile numero di pixel per le immagini (per quanto a bassa risoluzione), applicheremo l'MLP a dei dati trasformati tramite PCA.\n",
    "\n",
    "**ATTENZIONE:** per dettagli sul dataset utilizzato, guardare le precedenti esercitazioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "married-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOTA BENE! *****\n",
    "# perché %matplotlib widget funzioni, installare nell'ambiente virtuale \n",
    "# il pacchetto ipympl con il comando:\n",
    "# pip install ipympl\n",
    "#\n",
    "# ATTENZIONE: perché funzioni è necessario chiudere e rilanciare jupyter-lab\n",
    "#\n",
    "# STILE DI VISUALIZZAZIONE PLOT FATTI CON MATPLOTLIB\n",
    "%matplotlib widget\n",
    "#\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from IPython.display import display\n",
    "\n",
    "# Il codice presente di seguito serve nel caso si verifichi un errore del tipo\n",
    "#\n",
    "# \"URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1124)>\"\n",
    "#\n",
    "# al momento di chiamare la funzione fetch_lfw_people di sklearn.datasets\n",
    "#\n",
    "# ATTENZIONE: il codice di seguito non è quindi sempre necessario; se non lo fosse, commentarlo pure.\n",
    "#\n",
    "\n",
    "import os, ssl\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-mexican",
   "metadata": {},
   "source": [
    "## Importazione del Dataset e Creazione di Training, Validation e Test set\n",
    "\n",
    "Importiamo il dataset $\\mathcal{D}$ da scikit-learn e dividiamolo in $\\mathcal{T}$, $\\mathcal{V}$ e $\\mathcal{P}$. Utilizzare le seguenti percentuali:\n",
    "1. $|\\mathcal{T}| = 40\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{V}| = 10\\% |\\mathcal{D}|$\n",
    "1. $|\\mathcal{P}| = 50\\% |\\mathcal{D}|$\n",
    "\n",
    "**ATTENZIONE:** visto che la classe MLPClassifier lo esegue in automatico, _NON_ sarà necessario trasformare le classi secondo la codifica del one-hot encoding.\n",
    "\n",
    "**ESERCIZIO:** completare il codice nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equivalent-devon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_trainval</th>\n",
       "      <th>X_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N. sanmples</th>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N.features</th>\n",
       "      <td>1850</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             X_trainval  X_test\n",
       "N. sanmples         644     644\n",
       "N.features         1850    1850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "face_data = lfw_people['data']\n",
    "face_images = lfw_people['images']\n",
    "face_tnames = lfw_people['target_names']\n",
    "face_targets = lfw_people['target']\n",
    "\n",
    "# Creare gli X_trainval, y_trainval, X_test, y_test\n",
    "# (RICORDA: il validation set viene creato \"internamente\" dalla classe MLPClassifier. \n",
    "# Gli deve essere solamente specificata la percentuale rispetto al training set)\n",
    "\n",
    "random_state = 20210527\n",
    "np.random.seed(random_state)\n",
    "\n",
    "test_p = 0.5\n",
    "val_p = 0.2  # Percentuale di dati di X_trainval da usare come validation set\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(face_data, face_targets, test_size=test_p, \n",
    "                                                          random_state=random_state, shuffle=True)\n",
    "\n",
    "display(pd.DataFrame({'X_trainval': X_trainval.shape, 'X_test': X_test.shape}, index=['N. sanmples', 'N.features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-settle",
   "metadata": {},
   "source": [
    "### PCA sul Training e Validation Set\n",
    "\n",
    "Prepariamo una PCA rispettoa i dati di *X_trainval*. Non considerimo quelli di *X_test* poiché simulato dati non a disposizione.\n",
    "\n",
    "Selezionare un numero di PC che spieghino il $95\\%$ della varianza totale dei dati.\n",
    "\n",
    "**NOTA:** A voler essere precisi, ancora meglio sarebbe stato preparare la PCA solo sui dati di training, escludendo quelli di validation. Tuttavia (per quanto mi risulta) la classe _MLPClassifier_ accetta solamente la fraione di training da usare come validation, non direttamente dei dati. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "precious-first",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero PC</th>\n",
       "      <th>% Varianza Tot. Spiegata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_trainval</th>\n",
       "      <td>123</td>\n",
       "      <td>0.950428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Numero PC  % Varianza Tot. Spiegata\n",
       "X_trainval        123                  0.950428"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparazione PCA\n",
    "\n",
    "pca = PCA(0.95)\n",
    "\n",
    "pca.fit(X_trainval)\n",
    "\n",
    "display(pd.DataFrame({'Numero PC': pca.n_components_, \n",
    "                      '% Varianza Tot. Spiegata': pca.explained_variance_ratio_.sum()}, \n",
    "                     index=['X_trainval']))\n",
    "\n",
    "# Trasformazione dati. Salvare i vecchi in \"copie di backup\"\n",
    "\n",
    "X_trainval_old = X_trainval.copy()\n",
    "X_trainval = pca.transform(X_trainval)\n",
    "\n",
    "X_test_old = X_test.copy()\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-trust",
   "metadata": {},
   "source": [
    "### Costruzione e Addestramento MLP\n",
    "\n",
    "Costruiamo un MLP caratterizzato da 5 hidden layers con 256 unità ognuno e funzione di attivazione _relu_.\n",
    "\n",
    "Come parametri di addestramento, si utilizzino quelli qui indicati:\n",
    "1. **Batch size**: 4;\n",
    "2. **Massimo numero di epoche**: 5000;\n",
    "3. **Early stopping**: pazienza di 75 epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "musical-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione iper-parametri MLP\n",
    "hidden_layer_sizes = [256] * 5\n",
    "activation = 'relu'\n",
    "patience = 75\n",
    "max_epochs = 5000\n",
    "verbose = False\n",
    "batch_sz = 4\n",
    "\n",
    "# Inizializzazione MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                    activation=activation,\n",
    "                    early_stopping=True,\n",
    "                    n_iter_no_change=patience,\n",
    "                    max_iter=max_epochs,\n",
    "                    validation_fraction=val_p,\n",
    "                    batch_size=batch_sz,\n",
    "                    verbose=verbose,\n",
    "                    random_state=random_state,\n",
    "                    solver='adam',\n",
    "                    # learning_rate='adaptive'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "miniature-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=4, early_stopping=True,\n",
       "              hidden_layer_sizes=[256, 256, 256, 256, 256], max_iter=5000,\n",
       "              n_iter_no_change=75, random_state=20210527,\n",
       "              validation_fraction=0.2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addestramento MLP\n",
    "mlp.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "three-sociology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train. + val.</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.958028</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.956402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.757878</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.746528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision    Recall        F1\n",
       "train. + val.  0.956522   0.958028  0.956522  0.956402\n",
       "test           0.748447   0.757878  0.748447  0.746528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>225</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon                 18            11                4              4   \n",
       "Colin Powell                  3            87                5             12   \n",
       "Donald Rumsfeld               2             5               42              7   \n",
       "George W Bush                 1             1               20            225   \n",
       "Gerhard Schroeder             0             3                1              5   \n",
       "Hugo Chavez                   0             2                3              9   \n",
       "Tony Blair                    0             3                5              3   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                       1            2           3  \n",
       "Colin Powell                       2            1           1  \n",
       "Donald Rumsfeld                    9            0           3  \n",
       "George W Bush                      4            1           8  \n",
       "Gerhard Schroeder                 37            1           3  \n",
       "Hugo Chavez                        5           21           1  \n",
       "Tony Blair                         8            0          52  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.069767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.009009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.024390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.418605      0.255814         0.093023       0.093023   \n",
       "Colin Powell           0.027027      0.783784         0.045045       0.108108   \n",
       "Donald Rumsfeld        0.029412      0.073529         0.617647       0.102941   \n",
       "George W Bush          0.003846      0.003846         0.076923       0.865385   \n",
       "Gerhard Schroeder      0.000000      0.060000         0.020000       0.100000   \n",
       "Hugo Chavez            0.000000      0.048780         0.073171       0.219512   \n",
       "Tony Blair             0.000000      0.042254         0.070423       0.042254   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.023256     0.046512    0.069767  \n",
       "Colin Powell                0.018018     0.009009    0.009009  \n",
       "Donald Rumsfeld             0.132353     0.000000    0.044118  \n",
       "George W Bush               0.015385     0.003846    0.030769  \n",
       "Gerhard Schroeder           0.740000     0.020000    0.060000  \n",
       "Hugo Chavez                 0.121951     0.512195    0.024390  \n",
       "Tony Blair                  0.112676     0.000000    0.732394  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.776786</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.033962</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.750000      0.098214           0.0500       0.015094   \n",
       "Colin Powell           0.125000      0.776786           0.0625       0.045283   \n",
       "Donald Rumsfeld        0.083333      0.044643           0.5250       0.026415   \n",
       "George W Bush          0.041667      0.008929           0.2500       0.849057   \n",
       "Gerhard Schroeder      0.000000      0.026786           0.0125       0.018868   \n",
       "Hugo Chavez            0.000000      0.017857           0.0375       0.033962   \n",
       "Tony Blair             0.000000      0.026786           0.0625       0.011321   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.015152     0.076923    0.042254  \n",
       "Colin Powell                0.030303     0.038462    0.014085  \n",
       "Donald Rumsfeld             0.136364     0.000000    0.042254  \n",
       "George W Bush               0.060606     0.038462    0.112676  \n",
       "Gerhard Schroeder           0.560606     0.038462    0.042254  \n",
       "Hugo Chavez                 0.075758     0.807692    0.014085  \n",
       "Tony Blair                  0.121212     0.000000    0.732394  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance\n",
    "\n",
    "y_pred_trainval = mlp.predict(X_trainval)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "acc_trainval = mlp.score(X_trainval, y_trainval)\n",
    "prec_trainval = precision_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "rec_trainval = recall_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "f1_trainval = f1_score(y_trainval, y_pred_trainval, average='weighted')\n",
    "\n",
    "acc = mlp.score(X_test, y_test)\n",
    "prec = precision_score(y_test, y_pred, average='weighted')\n",
    "rec = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "df_perf = pd.DataFrame({'Accuracy': [acc_trainval, acc], \n",
    "                        'Precision': [prec_trainval, prec], \n",
    "                        'Recall': [rec_trainval, rec],\n",
    "                        'F1': [f1_trainval, f1]\n",
    "                       },\n",
    "                      index=['train. + val.', 'test'])\n",
    "\n",
    "cmat = confusion_matrix(y_test, y_pred, labels=mlp.classes_)\n",
    "cmat_norm_true = confusion_matrix(y_test, y_pred, labels=mlp.classes_, normalize='true')\n",
    "cmat_norm_pred = confusion_matrix(y_test, y_pred, labels=mlp.classes_, normalize='pred')\n",
    "\n",
    "df_cmat = pd.DataFrame(cmat, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_true = pd.DataFrame(cmat_norm_true, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_pred = pd.DataFrame(cmat_norm_pred, columns=face_tnames, index=face_tnames)\n",
    "\n",
    "display(df_perf)\n",
    "display(df_cmat)\n",
    "display(df_cmat_norm_true)\n",
    "display(df_cmat_norm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-steps",
   "metadata": {},
   "source": [
    "## Alcuni Esempi Visivi\n",
    "\n",
    "Mostriamo visivamente come viene fatta la classificazione multi-classe. \n",
    "\n",
    "**RICORDA:** la classe MLPClassifier ha un metodo _predict_proba_ che restituisce le probabilità di appartenenza di un input ad ognuna delle classi. Quella con probabilità massima è poi quella predetta in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "encouraging-pendant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8595adbb04248c1b15772b6b122db86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4daf3f8192346f290c116f67a74e9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3234fd22ad49259fae93e940179e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a89cff7542a4abd89709e4099330ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2df1dc387d4102b40ea9957433aae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f10a70ceffa4b1cbbd68f192c07d071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b1c25de69e49e2a72b725a1a8c0f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e811c4f66b7a435480ba6149b91dd88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66680c4d4744dcab5eb74ba8f948303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e52841780d4a8191be610c23793db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fdfb7251af4d61b1d55f4468a0fe3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c060e0ac72c4ed3a0372ae88b0e37d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6d54a7d63e4d9cac53da81181867b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff006e3deb04ac8bace6a95e098f937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287175511daf49afa059b9a3030e9000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31261aa376e54ebba7dff653025eccd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da03efdb16394033abfa190df1680273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e0a0d85b9b43b28a4b3cea90b3fead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51a39880beb4659ac2d777044624415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b66362740784bc7b1b48b0173861ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-71532d66811c>:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6b622075b94d6f828e90e9af11e0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb073180230640809eef595d943dd2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f65649940824ae5b1416063baf63cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82489917233846219e4fe85661651943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9c6262b4204d10aac2f1ac323753c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Abbreviazione nomi per etichette in barplot\n",
    "face_tnames_short = []\n",
    "for name in face_tnames:\n",
    "    name_split = name.split(' ')\n",
    "    nm = ''\n",
    "    for word in name_split:\n",
    "        nm = nm + word[0]\n",
    "    face_tnames_short.append(nm)\n",
    "\n",
    "# Selezione di \"n_randsamples\" volti random dal dataset\n",
    "\n",
    "n_randsamples = 25\n",
    "ind_test_rand = np.random.choice(len(y_test), n_randsamples, replace=False)\n",
    "\n",
    "# Matrice delle n_randsamples volti scelti (una riga, un volto)\n",
    "rand_faces = X_test_old[ind_test_rand, :]\n",
    "rand_targets = y_test[ind_test_rand]\n",
    "\n",
    "# Decision Function per i volti random:\n",
    "rand_faces_decision = mlp.predict_proba(pca.transform(rand_faces))\n",
    "y_pred_rand_faces = mlp.predict(pca.transform(rand_faces))\n",
    "\n",
    "for i in range(n_randsamples):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ii = ind_test_rand[i]\n",
    "    face_ii = face_images[ii]\n",
    "    \n",
    "    axs[0].imshow(face_ii, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Volto {} ({})'.format(ii, face_tnames[face_targets[ii]]))\n",
    "    \n",
    "    axs[1].bar(np.arange(len(face_tnames)),\n",
    "               rand_faces_decision[i, :]\n",
    "              )\n",
    "    axs[1].grid()\n",
    "    axs[1].set_xticks(np.arange(len(face_tnames)))\n",
    "    axs[1].set_xticklabels(face_tnames_short,\n",
    "                           rotation=15,\n",
    "                           fontsize=12\n",
    "                          )\n",
    "    axs[1].set_title('Predizione: {}'.format(face_tnames[y_pred_rand_faces[i]]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-services",
   "metadata": {},
   "source": [
    "## \"Bilanciamendo\" il Training (+ Validation) Set\n",
    "\n",
    "Ingrandiamo virtualmente il dataset riflettendo le immagini rispetto al proprio asse verticale (vedi https://numpy.org/doc/stable/reference/generated/numpy.flip.html).\n",
    "\n",
    "Dopodiché, cerchiamo di costruire un \"training + validation\" set \"bilanciato\" cioè con una presenza equivalente delle varie classi. Per fare ciò esistono differenti approcci, noi opteremo per il seguente:\n",
    "1. Calcoliamo il numero $N_{\\min}$ di immagini della classe meno frequente nel dataset (esteso)\n",
    "2. Creiamo il training + validation set prendendo randomicamente $(1/2)N_{\\min}$ campioni da ogni classe.\n",
    "\n",
    "Per questo nuovo caso, prenderemo assumeremo un validation set con cardinalità pari al 10% dei dati usati in addestramento, cioè $|\\mathcal{V}| = 10\\% (|\\mathcal{T}| + |\\mathcal{V}|)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "running-watson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George W Bush</td>\n",
       "      <td>3</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colin Powell</td>\n",
       "      <td>1</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tony Blair</td>\n",
       "      <td>6</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Rumsfeld</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gerhard Schroeder</td>\n",
       "      <td>4</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ariel Sharon</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hugo Chavez</td>\n",
       "      <td>5</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  target  counts\n",
       "0      George W Bush       3    1060\n",
       "1       Colin Powell       1     472\n",
       "2         Tony Blair       6     288\n",
       "3    Donald Rumsfeld       2     242\n",
       "4  Gerhard Schroeder       4     218\n",
       "5       Ariel Sharon       0     154\n",
       "6        Hugo Chavez       5     142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_min</th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142</td>\n",
       "      <td>Hugo Chavez</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_min         name  target\n",
       "0    142  Hugo Chavez       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainval_ext_size</th>\n",
       "      <th>test_ext_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>2079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trainval_ext_size  test_ext_size\n",
       "0                497           2079"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione dataset \"esteso\"\n",
    "\n",
    "face_images_flip = lfw_people['images'].copy()\n",
    "face_data_flip = lfw_people['data'].copy()\n",
    "face_targets_flip = face_targets.copy()\n",
    "\n",
    "for i in range(face_images_flip.shape[0]):\n",
    "    face_images_flip[i] = np.flip(face_images_flip[i], axis=1)  # specchio l'immagine risp. asse verticale\n",
    "    face_data_flip[i] = face_images_flip[i].flatten()  # vettorializzo l'immagine specchiata\n",
    "    \n",
    "face_images_ext = np.concatenate([face_images, face_images_flip], axis=0)\n",
    "face_data_ext = np.concatenate([face_data, face_data_flip], axis=0)\n",
    "face_targets_ext = np.concatenate([face_targets, face_targets_flip], axis=0)\n",
    "\n",
    "\n",
    "# DataFrame delle classi di appartenenza e dei nomi (dataset esteso)\n",
    "face_targets_ext_df = pd.DataFrame({'target': face_targets_ext, 'target_names': [face_tnames[t] for t in face_targets_ext]})\n",
    "face_counts_ext = face_targets_ext_df['target_names'].value_counts()\n",
    "\n",
    "Nmin = face_counts_ext.min()\n",
    "name_Nmin = face_counts_ext.index[face_counts_ext.argmin()]\n",
    "target_Nmin = list(face_tnames).index(name_Nmin)\n",
    "\n",
    "display(pd.DataFrame({'name': face_counts_ext.index, \n",
    "                      'target': [list(face_tnames).index(name) for name in face_counts_ext.index], \n",
    "                      'counts':face_counts_ext.values}\n",
    "                    )\n",
    "       )\n",
    "\n",
    "display(pd.DataFrame({'N_min': Nmin, 'name': name_Nmin, 'target': target_Nmin}, index=[0]))\n",
    "\n",
    "# Creazione nuovo training + validation set (per semplicità, tralasciamo l'array con le immagini)\n",
    "\n",
    "Nmin_half = int(np.round(Nmin * 0.5))\n",
    "\n",
    "indsX_trainval_ext_list = []\n",
    "indsX_test_ext_list = []\n",
    "\n",
    "for t in range(len(face_tnames)):\n",
    "    indsX_ext_t = np.argwhere(face_targets_ext == t).flatten()  # indici corrispondenti a immagini con target t\n",
    "    \n",
    "    indsX_trainval_ext_t, indsX_test_ext_t = train_test_split(indsX_ext_t, \n",
    "                                                              train_size=Nmin_half,\n",
    "                                                              random_state=random_state, \n",
    "                                                              shuffle=True)\n",
    "    \n",
    "    indsX_trainval_ext_list.append(indsX_trainval_ext_t)\n",
    "    indsX_test_ext_list.append(indsX_test_ext_t)\n",
    "\n",
    "indsX_trainval_ext = np.concatenate(indsX_trainval_ext_list, axis=0)\n",
    "indsX_test_ext = np.concatenate(indsX_test_ext_list, axis=0)\n",
    "\n",
    "# Mescoliamo (giusto per sicurezza) gli indici\n",
    "np.random.shuffle(indsX_trainval_ext)\n",
    "np.random.shuffle(indsX_test_ext)\n",
    "\n",
    "X_trainval_ext = face_data_ext[indsX_trainval_ext, :]\n",
    "y_trainval_ext = face_targets_ext[indsX_trainval_ext]\n",
    "\n",
    "X_test_ext = face_data_ext[indsX_test_ext, :]\n",
    "y_test_ext = face_targets_ext[indsX_test_ext]\n",
    "\n",
    "display(pd.DataFrame({'trainval_ext_size': len(y_trainval_ext), 'test_ext_size': len(y_test_ext)}, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-ceremony",
   "metadata": {},
   "source": [
    "### Nuovo Addestramento e Analisi delle Nuove Performance\n",
    "\n",
    "Di seguito ripetiamo quanto già fatto in precedenza, ma rispetto al nuovo dataset \"esteso e con training + validation set bilanciato\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forced-syntax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero PC</th>\n",
       "      <th>% Varianza Tot. Spiegata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_trainval_ext</th>\n",
       "      <td>117</td>\n",
       "      <td>0.950333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Numero PC  % Varianza Tot. Spiegata\n",
       "X_trainval_ext        117                  0.950333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparazione PCA\n",
    "\n",
    "pca_ext = PCA(0.95)\n",
    "\n",
    "pca_ext.fit(X_trainval_ext)\n",
    "\n",
    "display(pd.DataFrame({'Numero PC': pca_ext.n_components_, \n",
    "                      '% Varianza Tot. Spiegata': pca_ext.explained_variance_ratio_.sum()}, \n",
    "                     index=['X_trainval_ext']))\n",
    "\n",
    "# Trasformazione dati. Salvare i vecchi in \"copie di backup\"\n",
    "\n",
    "X_trainval_ext_old = X_trainval_ext.copy()\n",
    "X_trainval_ext = pca_ext.transform(X_trainval_ext)\n",
    "\n",
    "X_test_ext_old = X_test_ext.copy()\n",
    "X_test_ext = pca_ext.transform(X_test_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-eclipse",
   "metadata": {},
   "source": [
    "### Costruzione e Addestramento MLP\n",
    "\n",
    "Costruiamo un MLP caratterizzato da 5 hidden layers con 256 unità ognuno e funzione di attivazione _relu_.\n",
    "\n",
    "Come parametri di addestramento, si utilizzino quelli qui indicati:\n",
    "1. **Batch size**: 4;\n",
    "2. **Massimo numero di epoche**: 5000;\n",
    "3. **Early stopping**: pazienza di 75 epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attached-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione iper-parametri MLP\n",
    "hidden_layer_sizes = [256] * 5\n",
    "activation = 'relu'\n",
    "patience = 75\n",
    "max_epochs = 5000\n",
    "verbose = False\n",
    "batch_sz = 4\n",
    "\n",
    "# Inizializzazione MLP\n",
    "mlp_ext = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        activation=activation,\n",
    "                        early_stopping=True,\n",
    "                        n_iter_no_change=patience,\n",
    "                        max_iter=max_epochs,\n",
    "                        validation_fraction=0.10,\n",
    "                        batch_size=batch_sz,\n",
    "                        verbose=verbose,\n",
    "                        random_state=random_state\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hispanic-lemon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=4, early_stopping=True,\n",
       "              hidden_layer_sizes=[256, 256, 256, 256, 256], max_iter=5000,\n",
       "              n_iter_no_change=75, random_state=20210527)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addestramento MLP\n",
    "mlp_ext.fit(X_trainval_ext, y_trainval_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generous-penetration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train. + val.</th>\n",
       "      <td>0.979879</td>\n",
       "      <td>0.980282</td>\n",
       "      <td>0.979879</td>\n",
       "      <td>0.979860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.617124</td>\n",
       "      <td>0.736812</td>\n",
       "      <td>0.617124</td>\n",
       "      <td>0.641239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision    Recall        F1\n",
       "train. + val.  0.979879   0.980282  0.979879  0.979860\n",
       "test           0.617124   0.736812  0.617124  0.641239"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>28</td>\n",
       "      <td>277</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>95</td>\n",
       "      <td>552</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon                 57             5                7              1   \n",
       "Colin Powell                 28           277               15             15   \n",
       "Donald Rumsfeld              15            11              110              6   \n",
       "George W Bush                60            44               95            552   \n",
       "Gerhard Schroeder             3             3               11              3   \n",
       "Hugo Chavez                   1             2                0              0   \n",
       "Tony Blair                    4             7                8             10   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                       5            2           6  \n",
       "Colin Powell                      13           22          31  \n",
       "Donald Rumsfeld                    8            2          19  \n",
       "George W Bush                     54           80         104  \n",
       "Gerhard Schroeder                 75           26          26  \n",
       "Hugo Chavez                        3           61           4  \n",
       "Tony Blair                        19           18         151  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.069825</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.037406</td>\n",
       "      <td>0.037406</td>\n",
       "      <td>0.032419</td>\n",
       "      <td>0.054863</td>\n",
       "      <td>0.077307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.064327</td>\n",
       "      <td>0.643275</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.060667</td>\n",
       "      <td>0.044489</td>\n",
       "      <td>0.096057</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.054601</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.105157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.176871</td>\n",
       "      <td>0.176871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.056338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.036866</td>\n",
       "      <td>0.046083</td>\n",
       "      <td>0.087558</td>\n",
       "      <td>0.082949</td>\n",
       "      <td>0.695853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.686747      0.060241         0.084337       0.012048   \n",
       "Colin Powell           0.069825      0.690773         0.037406       0.037406   \n",
       "Donald Rumsfeld        0.087719      0.064327         0.643275       0.035088   \n",
       "George W Bush          0.060667      0.044489         0.096057       0.558140   \n",
       "Gerhard Schroeder      0.020408      0.020408         0.074830       0.020408   \n",
       "Hugo Chavez            0.014085      0.028169         0.000000       0.000000   \n",
       "Tony Blair             0.018433      0.032258         0.036866       0.046083   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.060241     0.024096    0.072289  \n",
       "Colin Powell                0.032419     0.054863    0.077307  \n",
       "Donald Rumsfeld             0.046784     0.011696    0.111111  \n",
       "George W Bush               0.054601     0.080890    0.105157  \n",
       "Gerhard Schroeder           0.510204     0.176871    0.176871  \n",
       "Hugo Chavez                 0.042254     0.859155    0.056338  \n",
       "Tony Blair                  0.087558     0.082949    0.695853  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <th>Colin Powell</th>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <th>George W Bush</th>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <th>Tony Blair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ariel Sharon</th>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.017595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colin Powell</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.793696</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.073446</td>\n",
       "      <td>0.104265</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Rumsfeld</th>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.031519</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.055718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George W Bush</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.940375</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.379147</td>\n",
       "      <td>0.304985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard Schroeder</th>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.008596</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.076246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hugo Chavez</th>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tony Blair</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>0.107345</td>\n",
       "      <td>0.085308</td>\n",
       "      <td>0.442815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ariel Sharon  Colin Powell  Donald Rumsfeld  George W Bush  \\\n",
       "Ariel Sharon           0.339286      0.014327         0.028455       0.001704   \n",
       "Colin Powell           0.166667      0.793696         0.060976       0.025554   \n",
       "Donald Rumsfeld        0.089286      0.031519         0.447154       0.010221   \n",
       "George W Bush          0.357143      0.126074         0.386179       0.940375   \n",
       "Gerhard Schroeder      0.017857      0.008596         0.044715       0.005111   \n",
       "Hugo Chavez            0.005952      0.005731         0.000000       0.000000   \n",
       "Tony Blair             0.023810      0.020057         0.032520       0.017036   \n",
       "\n",
       "                   Gerhard Schroeder  Hugo Chavez  Tony Blair  \n",
       "Ariel Sharon                0.028249     0.009479    0.017595  \n",
       "Colin Powell                0.073446     0.104265    0.090909  \n",
       "Donald Rumsfeld             0.045198     0.009479    0.055718  \n",
       "George W Bush               0.305085     0.379147    0.304985  \n",
       "Gerhard Schroeder           0.423729     0.123223    0.076246  \n",
       "Hugo Chavez                 0.016949     0.289100    0.011730  \n",
       "Tony Blair                  0.107345     0.085308    0.442815  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance\n",
    "\n",
    "y_pred_trainval_ext = mlp_ext.predict(X_trainval_ext)\n",
    "y_pred_ext = mlp_ext.predict(X_test_ext)\n",
    "\n",
    "acc_trainval_ext = mlp_ext.score(X_trainval_ext, y_trainval_ext)\n",
    "prec_trainval_ext = precision_score(y_trainval_ext, y_pred_trainval_ext, average='weighted')\n",
    "rec_trainval_ext = recall_score(y_trainval_ext, y_pred_trainval_ext, average='weighted')\n",
    "f1_trainval_ext = f1_score(y_trainval_ext, y_pred_trainval_ext, average='weighted')\n",
    "\n",
    "acc_ext = mlp_ext.score(X_test_ext, y_test_ext)\n",
    "prec_ext = precision_score(y_test_ext, y_pred_ext, average='weighted')\n",
    "rec_ext = recall_score(y_test_ext, y_pred_ext, average='weighted')\n",
    "f1_ext = f1_score(y_test_ext, y_pred_ext, average='weighted')\n",
    "\n",
    "df_perf_ext = pd.DataFrame({'Accuracy': [acc_trainval_ext, acc_ext], \n",
    "                            'Precision': [prec_trainval_ext, prec_ext], \n",
    "                            'Recall': [rec_trainval_ext, rec_ext],\n",
    "                            'F1': [f1_trainval_ext, f1_ext]\n",
    "                           },\n",
    "                           index=['train. + val.', 'test'])\n",
    "\n",
    "cmat_ext = confusion_matrix(y_test_ext, y_pred_ext, labels=mlp_ext.classes_)\n",
    "cmat_norm_true_ext = confusion_matrix(y_test_ext, y_pred_ext, labels=mlp_ext.classes_, normalize='true')\n",
    "cmat_norm_pred_ext = confusion_matrix(y_test_ext, y_pred_ext, labels=mlp_ext.classes_, normalize='pred')\n",
    "\n",
    "df_cmat_ext = pd.DataFrame(cmat_ext, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_true_ext = pd.DataFrame(cmat_norm_true_ext, columns=face_tnames, index=face_tnames)\n",
    "df_cmat_norm_pred_ext = pd.DataFrame(cmat_norm_pred_ext, columns=face_tnames, index=face_tnames)\n",
    "\n",
    "display(df_perf_ext)\n",
    "display(df_cmat_ext)\n",
    "display(df_cmat_norm_true_ext)\n",
    "display(df_cmat_norm_pred_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-analysis",
   "metadata": {},
   "source": [
    "## Alcuni Esempi Visivi\n",
    "\n",
    "Mostriamo visivamente come viene fatta la classificazione multi-classe. \n",
    "\n",
    "**RICORDA:** la classe MLPClassifier ha un metodo _predict_proba_ che restituisce le probabilità di appartenenza di un input ad ognuna delle classi. Quella con probabilità massima è poi quella predetta in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reliable-sound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cee88d4582641c98564817e85a17b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6036cd80abb4c4eb1f8a7677b03c22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1375c36eed43e9b13932ddeb2b533d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735eff2838ae4483bf87cc6f6b5be714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0fe48380d4478c99a2c1181ad776dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1cab0bc42e47cca38f69b14099d430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717f81f6dae54c2d99f336d73e906e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187ff1b9dc3c495b87da7154358f0eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b1a24ce9084daba26a1ebb0ecdd030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7047571edfb343dca892fb6f0b1d21fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c330f3c09e4482ba0b35e75881a31d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8900667977447f78302635e5c33bb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e87b9d186546558510bfa170d05e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b3554e99fd4a418af6ca17dfdb475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c527a22eb0b148fdb768c2c9dfa6897b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fc79a271454b978b4ed28b35e688b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1488d558bfaf4af68ae00409b17dc0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3df24eaaf6f424d8a46e9cd27738b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ada64ba3d954e498f5165f550822980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85e784933474f5da9b0fa1d48e7ff75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230b7742d9824725b7d8b5f1cb3f6cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a344d8ae4b54b7da66e236410bf752b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c0dc02e9d04775b689e71e4cfc2d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba25015aefe49ee8795007943d68d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2750692bb8704d5aa55e8d04d0e8bdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selezione di \"n_randsamples\" volti random dal dataset\n",
    "\n",
    "n_randsamples = 25\n",
    "ind_test_rand_ext = np.random.choice(len(y_test_ext), n_randsamples, replace=False)\n",
    "\n",
    "# Matrice delle n_randsamples volti scelti (una riga, un volto)\n",
    "rand_faces_ext = X_test_ext_old[ind_test_rand_ext, :]\n",
    "rand_targets_ext = y_test_ext[ind_test_rand_ext]\n",
    "\n",
    "# Decision Function per i volti random:\n",
    "rand_faces_decision_ext = mlp_ext.predict_proba(pca_ext.transform(rand_faces_ext))\n",
    "y_pred_rand_faces_ext = mlp_ext.predict(pca_ext.transform(rand_faces_ext))\n",
    "\n",
    "for i in range(n_randsamples):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ii = ind_test_rand_ext[i]\n",
    "    face_ii = face_images_ext[ii]\n",
    "    \n",
    "    axs[0].imshow(face_ii, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Volto {} ({})'.format(ii, face_tnames[face_targets_ext[ii]]))\n",
    "    \n",
    "    axs[1].bar(np.arange(len(face_tnames)),\n",
    "               rand_faces_decision_ext[i, :]\n",
    "              )\n",
    "    axs[1].grid()\n",
    "    axs[1].set_xticks(np.arange(len(face_tnames)))\n",
    "    axs[1].set_xticklabels(face_tnames_short,\n",
    "                           rotation=15,\n",
    "                           fontsize=12\n",
    "                          )\n",
    "    axs[1].set_title('Predizione: {}'.format(face_tnames[y_pred_rand_faces_ext[i]]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-craps",
   "metadata": {},
   "source": [
    "## Alcune Osservazioni sui Risultati\n",
    "\n",
    "Il problema di riconoscimento volti non è stato chiaramente risolto bene dal MLP (neanche con un \"bilanciamento\" del dataset).\n",
    "\n",
    "In entrambi i casi, si hanno chiaramente problemi di overfitting. Questo significa che, in assenza di maggiori dati, maggiori configurazioni per l'MLP andrebbero testate. Tuttavia, a causa dei limiti della classe _MLPClassifier_, non è garantito che si trovino dei buoni risultati.\n",
    "\n",
    "**ALCUNE OSSERVAZIONI ULTERIORI:**\n",
    "- Il caso con dataset \"bilanciato\" mostra una buona accuratezza per Hugo Chavez. molto probabilmente questo è dovuto al fatto che abbiamo preso per il training la metà dei volti a disposizione per quella classe. Ma si deve anche ricordare che avevamo prima esteso il dataset raddoppiando il numero di volti presenti tramite riflessione. L'MLP potrebbe aver quindi appreso meglio Hugo Chavez perché nel training+validation e nel test set ci sono più o meno gli stessi volti, ma riflessi.\n",
    "- In assenza di altre immagini e in presenza delle limitazioni della classe _MLPClassifier_ un possibile approccio al problema potrebbe essere quello di suddividerlo in più sottoproblemi di classificazione binaria (similmente alle SVM).\n",
    "- In generale, con questo esempio, vediamo come le NN siano strumenti molto potenti ma altrettanto sensibili nella fase di addestramento. Per problemi con pochi dati e non eccessivamente grandi per dimensioni dello spazio, metodi classici di ML (come le SVM) rimangono degli strumenti più che validi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-underground",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
