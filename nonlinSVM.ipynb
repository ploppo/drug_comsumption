{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arranged-trick",
   "metadata": {},
   "source": [
    "# SVM Non Lineari\n",
    "\n",
    "\n",
    "In questa esercitazione introdurremo l'utilizzo delle SVM nonlineari (per la classificazione) di scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-disposal",
   "metadata": {},
   "source": [
    "## Riassunto delle SVM Nonlineari (o Kernel SVM)\n",
    "\n",
    "Per informazioni più approfondite, consultare il capitolo 12 (sezione 4) del libro *Mathematics for Machine Learning* (https://mml-book.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-accused",
   "metadata": {},
   "source": [
    "### Idea alla Base delle Kernel SVM\n",
    "\n",
    "Quando si ha a che fare con dati i cui bordi di separazione tra le classi sono _altamente non lineari_, le SVM inevitabilmente falliscono (indipendentemente dal parametro $C$).\n",
    "\n",
    ">**IDEA:** \"Spostiamo\" il problema in uno spazio di dimensione superiore (denominato a volte _Spazio delle Features_) dove la separazione tra classi diventa lineare (o quasi).\n",
    "\n",
    "**IN PRATICA:** Dato il training set $\\mathcal{T} = \\{ (\\boldsymbol{x}_1,y_1),\\ldots , (\\boldsymbol{x}_T,y_T)\\}\\in\\mathbb{R}^n\\times \\{\\pm 1\\}$, scegliamo una mappa (tipicamente _non lineare_) arbitraria\n",
    "$$\\phi :\\mathbb{R}^n\\rightarrow \\mathbb{R}^m\\,, \\quad \\text{con }m>n\\,,$$\n",
    "e risolviamo il problema con una SVM rispetto al _nuovo training set_ \n",
    "$$\\mathcal{T}^\\phi = \\{ \\left(\\boldsymbol{\\varphi}_1,y_1\\right),\\ldots , \\left(\\boldsymbol{\\varphi}_T),y_T\\right)\\}\\in\\mathbb{R}^m\\times \\{\\pm 1\\}\\,, \\quad \\text{con }\\boldsymbol{\\varphi}_i = \\phi(\\boldsymbol{x}_i),\\, \\forall \\ i=1,\\ldots ,T\\,, $$\n",
    "\"sperando\" che ora le classi siano diventate _linearmente separabili_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-indonesia",
   "metadata": {},
   "source": [
    "### I Problemi del Passaggio allo Spazio delle Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-result",
   "metadata": {},
   "source": [
    "**Problema 1:** Aumentare la dimensione, comporta necessariamente un _aggravio dei costi computazionali_! \n",
    "\n",
    "**Esempio - Problema 1:** Prendiamo le soluzioni $\\boldsymbol{w}^*\\in\\mathbb{R}^m$ e $b\\in\\mathbb{R}$ del problema di ottimizzazione per le SVM _lineari_ risolto rispetto $\\mathcal{T}^\\phi$. Avremo allora:\n",
    "$$\\boldsymbol{w}^* = \\sum_{i\\in\\mathcal{I}_{\\mathrm{sv}}} y_i\\alpha_i^* \\boldsymbol{\\varphi}_i$$\n",
    "e\n",
    "$$b^* = \\frac{1}{|\\mathcal{I}_{\\mathrm{sv}}|} \\sum_{i\\in\\mathcal{I}_{\\mathrm{sv}}} (y_i - \\langle\\boldsymbol{w}^*, \\boldsymbol{\\varphi}_i\\rangle)\\,;$$\n",
    "\n",
    "Per calcolare la predizione $\\widehat{y}$ di un generico $\\boldsymbol{x}$ del test set (dove $\\boldsymbol{\\varphi}:=\\phi(\\boldsymbol{x})$), dovremo quindi calcolare:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\widehat{y} &= \\mathrm{sign}\\left( \\langle \\boldsymbol{w}^* , \\phi(\\boldsymbol{x})\\rangle + b^*\\right) = \\\\\n",
    "& = \\mathrm{sign}\\left(\n",
    "\\sum_{i\\in\\mathcal{I}_{\\mathrm{sv}}} y_i\\alpha_i^* \\langle \\boldsymbol{\\varphi}_i , \\boldsymbol{\\varphi}\\rangle + \n",
    "\\frac{1}{|\\mathcal{I}_{\\mathrm{sv}}|} \\sum_{i\\in\\mathcal{I}_{\\mathrm{sv}}} \\left(y_i - \n",
    "\\sum_{j\\in\\mathcal{I}_{\\mathrm{sv}}} y_j\\alpha_j^* \\langle\\boldsymbol{\\varphi}_j ,\n",
    "\\boldsymbol{\\varphi}_i\\rangle \\right)\n",
    "\\right)\\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Nella formula sopra, il grosso dei calcoli è richiesto per i _prodotti scalari_ $\\langle \\boldsymbol{\\varphi}_i , \\boldsymbol{\\varphi}\\rangle$ e $\\langle\\boldsymbol{\\varphi}_j ,\\boldsymbol{\\varphi}_i\\rangle$, che vanno svolti in $\\mathbb{R}^m$, dove presumibilmente $m\\gg n$ (teoricamente anche $m=\\infty$). Oltretutto, questo problema non è ristretto alle predizioni per il test set, ma si estende anche ai metodi di risoluzione dei problemi di ottimizzazione delle SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-genre",
   "metadata": {},
   "source": [
    "**Problema 2:** Quale mappa $\\phi :\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ devo scegliere per una buona separazione _lineare_ in $\\mathbb{R}^m$ e dei calcoli \"leggeri\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-columbus",
   "metadata": {},
   "source": [
    "### Il \"Kernel Trick\"\n",
    "\n",
    "La soluzione ai problemi sopra elencati è il cosiddetto \"_kernel trick_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-month",
   "metadata": {},
   "source": [
    "Sia $\\mathcal{X}\\subseteq\\mathbb{R}^n$ il dominio dei vettori $\\boldsymbol{x}$ del mio problema di classificazione.\n",
    "\n",
    "In poche parole, un _Kernel_ $k$ è una funzione \n",
    "\n",
    "$$k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow\\mathbb{R}$$\n",
    "\n",
    "per cui esiste un'unica mappa \n",
    "\n",
    "$$\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\subseteq\\mathbb{R}^m\\,,$$\n",
    "\n",
    "con $m>n$ ed $\\mathcal{H}$ spazio di _Hilbert_, per cui il prodotto scalare in $\\mathcal{H}$ di $\\boldsymbol{\\varphi}_i=\\phi(\\boldsymbol{x}_i)$ e $\\boldsymbol{\\varphi}_j=\\phi(\\boldsymbol{x}_j)$, per ogni $\\boldsymbol{x}_i,\\boldsymbol{x}_j\\in\\mathcal{X}$, è definito da $k$; in altre parole:\n",
    "\n",
    "$$\\langle\\boldsymbol{\\varphi}_i,\\boldsymbol{\\varphi}_j\\rangle_{\\mathcal{H}} = k(\\boldsymbol{x}_i,\\boldsymbol{x}_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-technician",
   "metadata": {},
   "source": [
    "**Nota bene:** I vantaggi principali garantiti da un kernel $k$ definito come sopra sono i seguenti.\n",
    "1. **Calcoli svolti in** $\\mathbb{R}^n$**:** Il costo computazionale dei prodotti scalari $\\langle \\cdot ,\\cdot\\rangle_{\\mathcal{H}}$ con vettori di $\\mathbb{R}^m$ si possono calcolare in $\\mathbb{R}^n$! Possiamo quindi scegliere un qualsiasi valore di $m>n$ senza appesantire i conti.\n",
    "2. **Non è necessario conoscere** $\\phi$ **esplicitamente:** La funzione $\\phi$ viene sempre coinvolta tramite il prodotto scalare in $\\mathcal{H}$, quindi non è necessario conoscere $\\phi$ se si conosce $k$. _NELLA PRATICA:_ si sceglie e si usa il _kernel_ $k$ ma non $\\phi$.\n",
    "\n",
    "**Osservazione:** Per le Kernel SVM si risolve generalmente il problema duale poiché i prodotti scalari $\\langle \\boldsymbol{w}, \\boldsymbol{w}\\rangle_{\\mathcal{H}}$ e $\\langle \\boldsymbol{w}, \\boldsymbol{x}\\rangle_{\\mathcal{H}}$ non possono essere calcolati facilmente tramite $k$, perché si dovrebbe disporre di $\\boldsymbol{x}_{\\boldsymbol{w}}=\\phi^{-1}(\\boldsymbol{w})$ (ammesso che esista)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-master",
   "metadata": {},
   "source": [
    "### I Kernel più Utilizzati\n",
    "\n",
    "Di seguito elenchiamo alcuni dei kernel più utilizzati per le SVM ed anche implementati in scikit-learn (vedi https://scikit-learn.org/stable/modules/svm.html#svm-kernels).\n",
    "\n",
    "1. **Kernel Lineare:** è il kernel corrispondente al caso in cui $\\phi$ sia l'identità (cioè è il kernel delle SVM lineari). Specificatamente:\n",
    "\\begin{equation}\n",
    "\\langle\\boldsymbol{\\varphi}_i, \\boldsymbol{\\varphi}_j\\rangle_{\\mathcal{H}} = \\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j\\rangle_{\\mathcal{X}}\\,,\n",
    "\\end{equation}\n",
    "dove (generalmente, ma non necessariamente) si ha che il prodotto scalare in $\\mathcal{X}$ è lo stesso di $\\mathbb{R}^n$ che induce la norma euclidea $||\\cdot||_2$, cioè $\\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j\\rangle_{\\mathcal{X}} = \\boldsymbol{x}_i^\\top \\boldsymbol{x}_j$.\n",
    "\n",
    "2. **Kernel Polinomiale:** è un kernel caratterizzato da un'espressione di tipo polinomiale.\n",
    "\\begin{equation}\n",
    "\\langle \\boldsymbol{\\varphi}_i, \\boldsymbol{\\varphi}_j\\rangle_{\\mathcal{H}} = \\left(\\gamma \\, \\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j\\rangle_{\\mathcal{X}} + c_0 \\right)^d\\,;\n",
    "\\end{equation}\n",
    "\n",
    "3. **Radial Basis Function (RBF) Kernel:** è un kernel caratterizzato da un'espressione di tipo esponenziale (tipicamente il più utilizzato).\n",
    "\\begin{equation}\n",
    "\\langle \\boldsymbol{\\varphi}_i, \\boldsymbol{\\varphi}_j\\rangle_{\\mathcal{H}} = e^{-\\gamma \\, \\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j\\rangle_{\\mathcal{X}}^2}\\,;\n",
    "\\end{equation}\n",
    "\n",
    "4. **Kernel \"Sigmoidale\":** è un kernel caratterizzato da un'espressione di tipo sigmoidale (facente uso della tangente iperbolica).\n",
    "\\begin{equation}\n",
    "\\langle \\boldsymbol{\\varphi}_i, \\boldsymbol{\\varphi}_j\\rangle_{\\mathcal{H}} = \\mathrm{tanh}\\left(\\gamma \\, \\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j\\rangle_{\\mathcal{X}} + c_0 \\right)\\,;\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-restriction",
   "metadata": {},
   "source": [
    "#### I Parametri dei Kernel\n",
    "\n",
    "I parametri $\\gamma$ e $c_0$ dei kernel influenzano l'addestramento delle SVM. \n",
    "\n",
    "In particolare, più grande è $\\gamma$, maggiore è l'influenza che hanno i singoli elementi del training set per l'addestramento. Il valore di $\\gamma$, su basi sperimentali, è generalmente impostato di default su valori pari a $1/n$ oppure $1/\\left(n \\cdot \\mathrm{Var}[(x_1^{(1)},\\ldots ,x_T^{(1)}, \\ldots ,x_1^{(n)}, \\ldots , x_T^{(n)})]\\right)$.\n",
    "\n",
    "Il parametro $c_0$, generalmente impostato su $0$, svolge un ruolo simile a gamma ma meno \"influente\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-numbers",
   "metadata": {},
   "source": [
    "# Esercitazione: Implementazione di SVM Non Lineari\n",
    "\n",
    "Nell'esercitazione di oggi utilizzeremo la classe *SVC* di scikit-learn, pensata per l'implementazione di Kernel SVM con *margine morbido*.\n",
    "\n",
    "**N.B.:** in scikit-learn non è prevista una classe per SVM senza margine morbido. Per simulare l'assenza di margine morbido è sempre possibile scegliere valori di $C$ molto alti.\n",
    "\n",
    "**FILE DA SCARICARE:** per le seguenti celle di codice è necessario avere scaricato dalla pagina del corso il dataset _nonlinear_1.csv_. Questo dataset è costituito da $500$ punti in $[-1, 2]^2\\subset\\mathbb{R}^2$ suddivisi in due classi tali che:\n",
    "\\begin{equation}\n",
    "\\text{class}(\\boldsymbol{x}) = \n",
    "\\begin{cases}\n",
    "1\\,,\\quad &\\text{se } \\ x_1^2 + \\frac{1}{2} \\leq x_2 \\ \\vee \\ ||\\boldsymbol{x} - [0, -\\frac{1}{4}]^\\top|| \\leq \\frac{1}{4}\\\\\n",
    "0\\,,\\quad &\\text{altrimenti}\n",
    "\\end{cases}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "certain-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOTA BENE! *****\n",
    "# perché %matplotlib widget funzioni, installare nell'ambiente virtuale \n",
    "# il pacchetto ipympl con il comando:\n",
    "# pip install ipympl\n",
    "#\n",
    "# ATTENZIONE: perché funzioni è necessario chiudere e rilanciare jupyter-lab\n",
    "#\n",
    "# STILE DI VISUALIZZAZIONE PLOT FATTI CON MATPLOTLIB\n",
    "%matplotlib widget\n",
    "#\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-gauge",
   "metadata": {},
   "source": [
    "## Importazione e Visualizzazione del Dataset\n",
    "\n",
    "**Esercizio:** Importare il dataset come un pandas DataFrame e visualizzare i punti tramite uno scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-principal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.302575</td>\n",
       "      <td>0.661118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115377</td>\n",
       "      <td>1.071897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.070935</td>\n",
       "      <td>-0.339227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.040557</td>\n",
       "      <td>-0.700399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.850840</td>\n",
       "      <td>0.867135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.960565</td>\n",
       "      <td>1.525409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-0.010775</td>\n",
       "      <td>-0.134782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.721830</td>\n",
       "      <td>1.760836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.852431</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.668482</td>\n",
       "      <td>-0.311024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2  class\n",
       "0    1.302575  0.661118      0\n",
       "1    0.115377  1.071897      1\n",
       "2    0.070935 -0.339227      1\n",
       "3    1.040557 -0.700399      0\n",
       "4   -0.850840  0.867135      0\n",
       "..        ...       ...    ...\n",
       "495  1.960565  1.525409      0\n",
       "496 -0.010775 -0.134782      1\n",
       "497  0.721830  1.760836      1\n",
       "498  0.852431  0.030409      0\n",
       "499 -0.668482 -0.311024      0\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importazione e visualizzazione del DataFrame come tabella tramite funzione display\n",
    "\n",
    "dataset = pd.read_csv('Dataset/nonlinear_1.csv')\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consecutive-sucking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe4887f6d884375af953cd19c02b0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Dataset \"nonlinear_1\"')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scatterplot dei punti del dataset\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(dataset.loc[:,'x1'],dataset.loc[:,'x2'], c=dataset.loc[:,'class'])\n",
    "plt.title('Dataset \"nonlinear_1\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-appraisal",
   "metadata": {},
   "source": [
    "### Utilizzo della Classe SVC\n",
    "\n",
    "**Esercizio:** Leggere la documentazione della classe *SVC* (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) e completare i codici delle celle seguenti.\n",
    "Per i plot, consultare la seguente documentazione:\n",
    "- https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html\n",
    "- https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\n",
    "- https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "- https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html\n",
    "\n",
    "**Suggerimenti/Indicazioni l'esercizio:**\n",
    "1. per lo svolgimento dell'esercizio focalizzarsi sui parametri di inizializzazione *C, kernel, degree, gamma, coef0, random_state*;\n",
    "3. leggere bene i metodi della classe;\n",
    "\n",
    "**RICORDA:** dati $\\boldsymbol{w}^*$ e $b^*$ parametri dell'iperpiano separatore ottimale $\\Pi^\\phi_{\\boldsymbol{w}^*,b^*}$ in $\\mathcal{H}\\subseteq\\mathbb{R}^m$, il margine è identificato da\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\langle\\boldsymbol{w}^* , \\phi(\\boldsymbol{x})\\rangle_{\\mathcal{H}} + b^* = 0\\}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "mentre i bordi del margine sono identificati dai due iperpiani\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{M}^+_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\langle\\boldsymbol{w}^* , \\phi(\\boldsymbol{x})\\rangle_{\\mathcal{H}} + b^* - 1 = 0\\}\n",
    "\\end{equation}\n",
    "e\n",
    "\\begin{equation}\n",
    "\\mathcal{M}^-_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\langle\\boldsymbol{w}^* , \\phi(\\boldsymbol{x})\\rangle_{\\mathcal{H}} + b^* + 1 = 0\\}\\,;\n",
    "\\end{equation}\n",
    "\n",
    "cioè sono identificati rispettivamente da quegli $\\boldsymbol{x}\\in\\mathbb{R}^n$ tali che $\\langle\\boldsymbol{w}^* , \\phi(\\boldsymbol{x})\\rangle_{\\mathcal{H}} + b^* =\\pm 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-submission",
   "metadata": {},
   "source": [
    "**N.B.:** Quanto ricordato sopra può essere utile se si volesse disegnare il bordo ed i margini quando $n=2$. Si può infatti sfruttare le \"funzioni di contorno\" di matplotlib ed il metodo \"_decision_function_\" della SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "physical-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suddivisione dataset in training e test set (50% training, 50% test)\n",
    "\n",
    "random_state = 20210513  # Random seed\n",
    "test_p = 0.50  # Percentuale di dati da utilizzare come test set\n",
    "\n",
    "X = dataset.loc[:,('x1','x2')]\n",
    "\n",
    "y = dataset['class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= test_p, random_state = random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welcome-level",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc. (C1)</th>\n",
       "      <th>acc. (C2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.980</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc. (C1)  acc. (C2)\n",
       "training      0.948      0.988\n",
       "test          0.980      1.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f98f783c854866b70e6fb64b3a0e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SVM RBF - C = 100.0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KERNEL RBF:\n",
    "# Addestrare due SVM con RBF kernel, rispetto a due valori di C (ma stesso gamma pari ad 1/n)\n",
    "\n",
    "ker_rbf = 'rbf'\n",
    "gamma_rbf = 'auto'\n",
    "\n",
    "C1 = 5\n",
    "C2 = 1e2\n",
    "\n",
    "# Inizializzazione SVM\n",
    "svm_rbf_1 = SVC(kernel=ker_rbf, gamma=gamma_rbf, C=C1, random_state=random_state)\n",
    "svm_rbf_2 = SVC(kernel=ker_rbf, gamma=gamma_rbf, C=C2, random_state=random_state)\n",
    "\n",
    "# Addestramento SVM\n",
    "svm_rbf_1.fit(X_train, y_train)\n",
    "svm_rbf_2.fit(X_train, y_train)\n",
    "\n",
    "# Dataframe con accuratezza su training e test set (visualizzare con funzione 'display')\n",
    "df_svm_rbf = pd.DataFrame({'acc. (C1)': [svm_rbf_1.score(X_train, y_train), svm_rbf_1.score(X_test, y_test)],\n",
    "                           'acc. (C2)': [svm_rbf_2.score(X_train, y_train), svm_rbf_2.score(X_test, y_test)]},\n",
    "                          index=['training', 'test'])\n",
    "display(df_svm_rbf)\n",
    "\n",
    "\n",
    "# Plot\n",
    "# ISTRUZIONI:\n",
    "# - raffigurare training e test set con 'marker' diversi dello scatterplot\n",
    "# - usare la funzione contour (dopo aver definito una mesh di valori con np.meshgrid) per disegnare bordo e margini\n",
    "\n",
    "minx = -1\n",
    "maxx = 2\n",
    "mesh_points = 200\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.linspace(minx, maxx, mesh_points), np.linspace(minx, maxx, mesh_points))\n",
    "XX = np.concatenate([xx1.reshape(mesh_points ** 2, 1), xx2.reshape(mesh_points ** 2, 1)], axis=1)\n",
    "\n",
    "ZZ_1 = svm_rbf_1.decision_function(XX) #matrice\n",
    "zz_1 = ZZ_1.reshape(mesh_points, mesh_points) #forma a griglia\n",
    "\n",
    "ZZ_2 = svm_rbf_2.decision_function(XX)\n",
    "zz_2 = ZZ_2.reshape(mesh_points, mesh_points)\n",
    "\n",
    "fig_rbf, axs_rbf = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axs_rbf[0].scatter(X_test.iloc[:,0],X_test.iloc[:,1], c=y_test)\n",
    "axs_rbf[0].scatter(X_train.loc[:,'x1'], X_train.loc[:,'x2'],c=y_train)\n",
    "axs_rbf[0].contour(xx1, xx2, zz_1, [0])\n",
    "axs_rbf[0].set_title('SVM RBF - C = {}'.format(C1))\n",
    "axs_rbf[1].scatter(X_test.iloc[:,0],X_test.iloc[:,1], c=y_test)\n",
    "axs_rbf[1].scatter(X_train.loc[:,'x1'], X_train.loc[:,'x2'],c=y_train)\n",
    "axs_rbf[1].contour(xx1, xx2, zz_2, [0])\n",
    "axs_rbf[1].set_title('SVM RBF - C = {}'.format(C2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-astrology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc. (C1)</th>\n",
       "      <th>acc. (C2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc. (C1)  acc. (C2)\n",
       "training      0.912      0.920\n",
       "test          0.920      0.912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de000100df6d4f3799f997ab5108d694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SVM POLY - C = 100.0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KERNEL POLINOMIALE:\n",
    "# Addestrare due SVM con RBF kernel, rispetto a due valori di C (ma stesso gamma pari ad 1/n, stesso grado pari a 3 e stesso c_0 = 0)\n",
    "\n",
    "ker_poly = 'poly'\n",
    "gamma = 'auto'\n",
    "\n",
    "C1 = 5\n",
    "C2 = 1e2\n",
    "\n",
    "# Inizializzazione SVM\n",
    "svm_rbf_1 = SVC(kernel=ker_poly, degree=3, gamma=gamma, C=C1, random_state=random_state, coef0=0)\n",
    "svm_rbf_2 = SVC(kernel=ker_poly, degree=3, gamma=gamma, C=C2, random_state=random_state, coef0=0)\n",
    "\n",
    "# Addestramento SVM\n",
    "svm_rbf_1.fit(X_train, y_train)\n",
    "svm_rbf_2.fit(X_train, y_train)\n",
    "\n",
    "# Dataframe con accuratezza su training e test set (visualizzare con funzione 'display')\n",
    "df_svm_rbf = pd.DataFrame({'acc. (C1)': [svm_rbf_1.score(X_train, y_train), svm_rbf_1.score(X_test, y_test)],\n",
    "                           'acc. (C2)': [svm_rbf_2.score(X_train, y_train), svm_rbf_2.score(X_test, y_test)]},\n",
    "                          index=['training', 'test'])\n",
    "display(df_svm_rbf)\n",
    "\n",
    "\n",
    "# Plot\n",
    "# ISTRUZIONI:\n",
    "# - raffigurare training e test set con 'marker' diversi dello scatterplot\n",
    "# - usare la funzione contour (dopo aver definito una mesh di valori con np.meshgrid) per disegnare bordo e margini\n",
    "\n",
    "minx = -1\n",
    "maxx = 2\n",
    "mesh_points = 200\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.linspace(minx, maxx, mesh_points), np.linspace(minx, maxx, mesh_points))\n",
    "XX = np.concatenate([xx1.reshape(mesh_points ** 2, 1), xx2.reshape(mesh_points ** 2, 1)], axis=1)\n",
    "\n",
    "ZZ_1 = svm_rbf_1.decision_function(XX) #matrice\n",
    "zz_1 = ZZ_1.reshape(mesh_points, mesh_points) #forma a griglia\n",
    "\n",
    "ZZ_2 = svm_rbf_2.decision_function(XX)\n",
    "zz_2 = ZZ_2.reshape(mesh_points, mesh_points)\n",
    "\n",
    "fig_rbf, axs_rbf = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axs_rbf[0].scatter(X_test.iloc[:,0],X_test.iloc[:,1], c=y_test)\n",
    "axs_rbf[0].scatter(X_train.loc[:,'x1'], X_train.loc[:,'x2'],c=y_train)\n",
    "axs_rbf[0].contour(xx1, xx2, zz_1, [0])\n",
    "axs_rbf[0].set_title('SVM POLY - C = {}'.format(C1))\n",
    "axs_rbf[1].scatter(X_test.iloc[:,0],X_test.iloc[:,1], c=y_test)\n",
    "axs_rbf[1].scatter(X_train.loc[:,'x1'], X_train.loc[:,'x2'],c=y_train)\n",
    "axs_rbf[1].contour(xx1, xx2, zz_2, [0])\n",
    "axs_rbf[1].set_title('SVM POLY - C = {}'.format(C2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "global-museum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc. (C1)</th>\n",
       "      <th>acc. (C2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.580</td>\n",
       "      <td>0.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.552</td>\n",
       "      <td>0.552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc. (C1)  acc. (C2)\n",
       "training      0.580      0.568\n",
       "test          0.552      0.552"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83af91009b9407ab8d5c6e09614f0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SVM SIGMOID - C = 100.0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KERNEL SIGMOIDALE:\n",
    "# Addestrare due SVM con RBF kernel, rispetto a due valori di C (ma stesso gamma pari ad 1/n e stesso c_0 = 0)\n",
    "\n",
    "ker = 'sigmoid'\n",
    "gamma = 'auto'\n",
    "\n",
    "C1 = 5\n",
    "C2 = 1e2\n",
    "\n",
    "# Inizializzazione SVM\n",
    "svm_rbf_1 = SVC(kernel=ker, gamma=gamma, C=C1, random_state=random_state, coef0=0)\n",
    "svm_rbf_2 = SVC(kernel=ker, gamma=gamma, C=C2, random_state=random_state, coef0=0)\n",
    "\n",
    "# Addestramento SVM\n",
    "svm_rbf_1.fit(X_train, y_train)\n",
    "svm_rbf_2.fit(X_train, y_train)\n",
    "\n",
    "# Dataframe con accuratezza su training e test set (visualizzare con funzione 'display')\n",
    "df_svm_rbf = pd.DataFrame({'acc. (C1)': [svm_rbf_1.score(X_train, y_train), svm_rbf_1.score(X_test, y_test)],\n",
    "                           'acc. (C2)': [svm_rbf_2.score(X_train, y_train), svm_rbf_2.score(X_test, y_test)]},\n",
    "                          index=['training', 'test'])\n",
    "display(df_svm_rbf)\n",
    "\n",
    "\n",
    "# Plot\n",
    "# ISTRUZIONI:\n",
    "# - raffigurare training e test set con 'marker' diversi dello scatterplot\n",
    "# - usare la funzione contour (dopo aver definito una mesh di valori con np.meshgrid) per disegnare bordo e margini\n",
    "\n",
    "minx = -1\n",
    "maxx = 2\n",
    "mesh_points = 200\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.linspace(minx, maxx, mesh_points), np.linspace(minx, maxx, mesh_points))\n",
    "XX = np.concatenate([xx1.reshape(mesh_points ** 2, 1), xx2.reshape(mesh_points ** 2, 1)], axis=1)\n",
    "\n",
    "ZZ_1 = svm_rbf_1.decision_function(XX) #matrice\n",
    "zz_1 = ZZ_1.reshape(mesh_points, mesh_points) #forma a griglia\n",
    "\n",
    "ZZ_2 = svm_rbf_2.decision_function(XX)\n",
    "zz_2 = ZZ_2.reshape(mesh_points, mesh_points)\n",
    "\n",
    "fig_rbf, axs_rbf = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axs_rbf[0].scatter(X_test.iloc[:,0],X_test.iloc[:,1], c=y_test)\n",
    "axs_rbf[0].scatter(X_train.loc[:,'x1'], X_train.loc[:,'x2'],c=y_train)\n",
    "axs_rbf[0].contour(xx1, xx2, zz_1, [0])\n",
    "axs_rbf[0].set_title('SVM SIGMOID - C = {}'.format(C1))\n",
    "axs_rbf[1].scatter(X_test.iloc[:,0],X_test.iloc[:,1], c=y_test)\n",
    "axs_rbf[1].scatter(X_train.loc[:,'x1'], X_train.loc[:,'x2'],c=y_train)\n",
    "axs_rbf[1].contour(xx1, xx2, zz_2, [0])\n",
    "axs_rbf[1].set_title('SVM SIGMOID - C = {}'.format(C2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-arctic",
   "metadata": {},
   "source": [
    "# Accuratezza, Precisione e Sensibilità\n",
    "\n",
    "In un problema di classificazione, l'accuratezza su un test set $\\mathcal{P}$, cioè la quantità\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{acc}(\\mathcal{P}) = \\frac{\\text{# predizioni corrette su }\\mathcal{P}}{|\\mathcal{P}|}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "non è sempre un indicatore esaustivo. Per esempio, si consideri un test set $\\mathcal{P}$ dove le classi $\\pm 1$ sono sbilanciate: 85% dei campioni appartenenti alla classe $+1$ ed il restante 15% alla classe $-1$. In questo caso, un modello che restituisce sempre $+1$, avrebbe comunque un'accuratezza dell'85%! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-florence",
   "metadata": {},
   "source": [
    "## Veri/Falsi Positivi/Negativi\n",
    "\n",
    "Prendiamo in considerazione un caso di classificazione _binaria_ con una classe positiva $+1$ ed una negativa $-1$.\n",
    "\n",
    "**ATTENZIONE:** Quanto diremo è facilmente generalizzabile a $c>2$ classi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-litigation",
   "metadata": {},
   "source": [
    "Oltre all'accuratezza, possiamo definire le seguenti quantità:\n",
    "- I **veri positivi** (_True Positive_, $TP$), cioè il _numero_ di campioni in $\\mathcal{P}$ che appartengono alla classe $+1$ e sono stati predetti correttamente come appartenenti ad essa;\n",
    "- I **veri negativi** (_True Negative_, $TN$), cioè il _numero_ di campioni in $\\mathcal{P}$ che appartengono alla classe $-1$ e sono stati predetti correttamente come appartenenti ad essa;\n",
    "- I **falsi positivi** (_False Positive_, $FP$), cioè il _numero_ di campioni in $\\mathcal{P}$ che appartengono alla classe $-1$ ma sono stati predetti come appartenenti alla classe $+1$;\n",
    "- I **falsi negativi** (_False Negative_, $FN$), cioè il _numero_ di campioni in $\\mathcal{P}$ che appartengono alla classe $+1$ ma sono stati predetti come appartenenti alla classe $-1$;\n",
    "\n",
    "**Osservazione:** $$\\mathrm{acc}(\\mathcal{P}) = \\frac{TP + TN}{TP + FP + TN + FN} = \\frac{TP + TN}{|\\mathcal{P}|}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-liberal",
   "metadata": {},
   "source": [
    "### La Matrice di Confusione\n",
    "\n",
    "Per capire meglio i _veri/falsi positivi/negativi_ si usa raffigurare la cosiddetta **matrice di confusione** $A$ (anche in questo caso facilmente estendibile a $c>2$ classi).\n",
    "\n",
    "Questa matrice, calcolata rispetto ad un test set $\\mathcal{P}$, ha come elemento $a_{ij}$ il numero di campioni appartenenti alla classe $C_i$ che sono stati predetti appartenere alla classe $C_j$.\n",
    "\n",
    "Nella classificazione binaria, definendo le classi $C_1 = +1$ e $C_2 = -1$, abbiamo $A\\in\\mathbb{R}^2$ ed in particolare:\n",
    "\n",
    "\\begin{equation}\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "TP & FN\\\\\n",
    "FP & TN\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "**Osservazione:** $$\\mathrm{acc}(\\mathcal{P}) = \\frac{\\mathrm{tr}(A)}{\\sum_{i,j=1}^c a_{ij}}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-pixel",
   "metadata": {},
   "source": [
    "### Precision e Recall\n",
    "\n",
    "Un altro impiego dei veri/falsi positivi/negativi è il calcolo di:\n",
    "- **Precision per una classe** $C_i$ **.** La _Precision_ indica, appunto, la precisione del modello nel classificare i campioni che appartengono alla classe $C_i$. Specificatamente:\n",
    "\\begin{equation}\n",
    "\\mathrm{prec}(C_i;\\mathcal{P}) = \\frac{\\text{veri }C_i}{\\text{veri }C_i + \\text{falsi }C_i} = \\frac{\\text{veri }C_i}{\\text{# di elem. in $\\mathcal{P}$ pred. $C_i$}}\\,.\n",
    "\\end{equation}\n",
    "Guardando alla matrice di confusione $A$, vale quindi\n",
    "\\begin{equation}\n",
    "\\mathrm{prec}(C_i;\\mathcal{P}) = \\frac{a_{ii}}{\\sum_{k=1}^c a_{ki}} = \\frac{a_{ii}}{\\text{somma elem. in col. $i$-esima}}\\,.\n",
    "\\end{equation}\n",
    "- **Recall per una classe** $C_i$ **.** La _Recall_ indica l'abilità del modello di trovare tutti i campioni della classe $C_i$ in $\\mathcal{P}$. Specificatamente:\n",
    "\\begin{equation}\n",
    "\\mathrm{rec}(C_i;\\mathcal{P}) = \\frac{\\text{veri }C_i}{\\text{# di } C_i \\text{ in }\\mathcal{P}}\\,.\n",
    "\\end{equation}\n",
    "Guardando alla matrice di confusione $A$, vale quindi\n",
    "\\begin{equation}\n",
    "\\mathrm{rec}(C_i;\\mathcal{P}) = \\frac{a_{ii}}{\\sum_{k=1}^c a_{ik}} = \\frac{a_{ii}}{\\text{somma elem. in riga $i$-esima}}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "**NOTA BENE:** In certi casi è comodo calcolare la precision e la recall medie (rispetto alle varie classi) per avere degli indicatori generali del modello predittivo. Questo viene fatto soprattutto nella classificazione binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-madness",
   "metadata": {},
   "source": [
    "### F-beta Score\n",
    "\n",
    "Una misura che aggrega le informazioni contenute nella _precision_ e nella _recall_ (rispetto ad una classe $C_i$) è l' $F_\\beta$ _score_. Questo è definito come:\n",
    "\n",
    "\\begin{equation}\n",
    "F_\\beta(C_i;\\mathcal{P}) = (1 + \\beta^2) \\ \\frac{\\mathrm{prec}(C_i;\\mathcal{P}) \\cdot \\mathrm{rec}(C_i;\\mathcal{P})}{\\beta^2 \\ \\mathrm{prec}(C_i;\\mathcal{P}) + \\mathrm{rec}(C_i;\\mathcal{P})}\n",
    "\\end{equation}\n",
    "\n",
    "I valori di $F_\\beta$ sono sempre in $[0, 1]$ e descrivono \"bontà\" e la \"robustezza\" delle predizioni del modello ($1$ massimo, $0$ minimo).\n",
    "\n",
    "**NOTA BENE:**\n",
    "- Nella pratica si misura generalmente l' $F_1$ score;\n",
    "- Come per la precision e la recall, anche per $F_\\beta$ si può calcolare il valore medio rispetto le varie classi (specialmente nella classificazione binaria)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-moisture",
   "metadata": {},
   "source": [
    "## Anticipazioni: Precision, Recall, F-beta Score in Scikit-Learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-calendar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-remove",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-premiere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-passion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-router",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-timothy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-treasure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-talent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-width",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-livestock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-transportation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-court",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-hygiene",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
