{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "removed-victor",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Lineari\n",
    "\n",
    "In questa esercitazione utilizzeremo alcune classi di scikit-learn per l'utilizzo di SVM lineari per la classificazione.\n",
    "\n",
    "## Riassunto sulle SVM\n",
    "\n",
    "Per informazioni più approfondite, consultare il capitolo 12 del libro *Mathematics for Machine Learning* (https://mml-book.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-identification",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Notazioni per l'esercitazione:**\n",
    "1. $\\mathcal{T} = \\{(\\boldsymbol{x}_1, y_1), \\ldots , (\\boldsymbol{x}_T, y_T) \\}\\subset \\mathbb{R}^n\\times \\{\\pm 1\\}$ è il training set costiutito da $T$ coppie $(\\boldsymbol{x}_i, y_i)$, con $y_i=\\pm 1$ rappresentante la classe del vettore $\\boldsymbol{x_i}\\in\\mathbb{R}^n$. \n",
    "\n",
    "2. Indichiamo rispettivamente l'insieme dei vettori e l'insieme delle classi in $\\mathcal{T}$ con $$X_{\\mathcal{T}}=\\{\\boldsymbol{x}_1,\\ldots ,\\boldsymbol{x}_T\\}$$ e $$Y_{\\mathcal{T}}=\\{y_1,\\ldots ,y_T\\}$$ gli insiemi dei vettori \n",
    "\n",
    "3. Indichiamo con $\\Pi_{\\boldsymbol{w},b}$ l'iperpiano di $\\mathbb{R}^n$ definito dal vettore normale $\\boldsymbol{w}\\in\\mathbb{R}^n$ e dal parametro $b\\in\\mathbb{R}$, cioè $$\\Pi_{\\boldsymbol{w},b}:= \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\boldsymbol{w}^\\top\\boldsymbol{x} + b = 0\\}\\,.$$ *N.B.: rispetto alle slide, abbiamo un cambio di notazione. Questa notazione alternativa segue quella usata da scikit-learn e dal libro sopra indicato, nel caso di una consultazione di questi ultimi durante l'esercitazione.* \n",
    "\n",
    "4. Indichiamo con $\\mathrm{dist}(\\Pi_{\\boldsymbol{w},b}, \\boldsymbol{x})$ la distanza euclidea tra un vettore $\\boldsymbol{x}\\in\\mathbb{R}^n$ e l'iperpiano $\\Pi_{\\boldsymbol{w},b}$. In particolare, ricordiamo che $$\\mathrm{dist}(\\Pi_{\\boldsymbol{w},b}, \\boldsymbol{x}) = \\frac{|\\boldsymbol{w}^\\top\\boldsymbol{x} + b|}{||\\boldsymbol{w}||}\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-wells",
   "metadata": {},
   "source": [
    "### Idea alla Base delle SVM\n",
    "\n",
    "L'idea alla base delle SVM è quella di trovare l'iperpiano che divide i vettori $\\boldsymbol{x}_1,\\ldots ,\\boldsymbol{x}_T$ nelle due classi di appartenenza $\\pm 1$, massimizzando l'ampiezza $M_{\\boldsymbol{w},b}$ del margine di separazione (vedi figura).\n",
    "\n",
    "**N.B.:** la separazione \"ottimale\" avviene imponendo che l'iperpiano sia equidistante dai vettori ad esso più vicini, per ognuna delle due classi; questo significa che le \"semi-ampiezze\" del margine devono essere uguali tra loro e pari alla distanza dei più vicini vettori all'iperpiano (per ognuna delle classi)\n",
    "\n",
    "<img src=\"figures/svm_margin.png\" width=\"480\">\n",
    "\n",
    "Matematicamente, l'ampiezza del margine $M_{\\boldsymbol{w},b}$ di un iperpiano $\\Pi_{\\boldsymbol{w},b}$ che divide due sottoinsiemi di vettori è quindi: $$M_{\\boldsymbol{w},b} = 2\\cdot \\min_{\\boldsymbol{x}}\\mathrm{dist}(\\Pi_{\\boldsymbol{w},b}, \\boldsymbol{x})\\,.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-artwork",
   "metadata": {},
   "source": [
    "Concettualmente, assumendo le classi $+1$ e $-1$ linearmente separabili, vogliamo quindi risolvere il problema\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\max_{\\boldsymbol{w},b} M_{\\boldsymbol{w},b}\\\\\n",
    "y_i \\frac{(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b)}{||\\boldsymbol{w}||} \\geq \\frac{M_{\\boldsymbol{w},b}}{2} \\,,\\quad \\forall \\ i=1,\\ldots ,T\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-password",
   "metadata": {},
   "source": [
    "### La Formulazione del Problema di Ottimizzazione nella Pratica\n",
    "\n",
    "Poiché per ogni scalare $k\\in\\mathbb{R}\\setminus\\{0\\}$ vale $\\Pi_{k\\boldsymbol{w},kb}\\equiv \\Pi_{\\boldsymbol{w}, b}$, possiamo restringere la ricerca dell'iperpiano separatore ottimale a quei parametri $\\boldsymbol{w}$ e $b$ tali che $$|\\boldsymbol{w}^\\top\\boldsymbol{x}+b|\\geq 1\\,, \\ \\forall \\, \\boldsymbol{x}\\in X_{\\mathcal{T}} \\,.$$\n",
    "\n",
    "**Definizioni:**\n",
    "1. iperpiani di questo tipo si definiscono *iperpiani canonici rispetto* $X_{\\mathcal{T}}$.\n",
    "\n",
    "2. i vettori $\\boldsymbol{x}\\in X_{\\mathcal{T}}$ tali che $|\\boldsymbol{w}^\\top\\boldsymbol{x}+b| = 1$ sono definiti *support vectors*.\n",
    "\n",
    "**Osservazioni:**\n",
    "1. se $\\Pi_{\\boldsymbol{w},b}$ è un iperpiano canonico rispetto $X_{\\mathcal{T}}$, allora $M_{\\boldsymbol{w},b}=\\frac{2}{||\\boldsymbol{w}||}$.\n",
    "\n",
    "2. restringendo il problema agli iperpiani canonici rispetto $X_{\\mathcal{T}}$, massimizzare $M_{\\boldsymbol{w},b} = 2 / ||\\boldsymbol{w}||$ è equivalente a minimizzare $\\frac{1}{2}||\\boldsymbol{w}||^{2} = \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-optimization",
   "metadata": {},
   "source": [
    "Il problema di ottimizzazione per trovare il separatore ottimale può essere quindi riscritto come:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\min_{\\boldsymbol{w}} \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w}\\\\\n",
    "y_i (\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\geq 1 \\,,\\quad \\forall \\ i=1,\\ldots ,T\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-particular",
   "metadata": {},
   "source": [
    "### Il Problema Duale\n",
    "\n",
    "Il problema di minimizzazione delle SVM sopra descritto viene risolto passando per il suo problema duale:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\min_{\\boldsymbol{\\alpha}} \\frac{1}{2}\\boldsymbol{\\alpha}^\\top Q \\boldsymbol{\\alpha} - \\sum_{i=1}^T \\alpha_i\\\\\n",
    "\\sum_{i=1}^T \\alpha_i y_i = 0\\\\\n",
    "\\alpha_i \\geq 0\\,,\\quad \\forall \\ i=1,\\ldots ,T\n",
    "\\end{cases}\\,,\n",
    "\\end{equation}\n",
    "dove $\\boldsymbol{\\alpha}\\in\\mathbb{R}^T$ e la matrice $Q\\in\\mathbb{R}^{T\\times T}$ è tale che\n",
    "\\begin{equation}\n",
    "Q = \\left(q_{i,j}\\right)_{i,j=1,\\ldots ,T} =  \\left( y_iy_j\\boldsymbol{x}_i^\\top\\boldsymbol{x}_j \\right)_{i,j=1,\\ldots ,T}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-flavor",
   "metadata": {},
   "source": [
    "**Osservazione:** Il problema duale considerato è un tipo di problema di ottimizzazione definito \"*programmazione quadratica*\", cioè con funzione obiettivo *quadratica* e vincoli *lineari*.\n",
    "\n",
    "**N.B.:** non è scopo di questo corso affrontare i dettagli dei problemi di ottimizzazione in questione né i metodi numerici per la loro risoluzione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-column",
   "metadata": {},
   "source": [
    "### Calcolo dell'Iperpiano Separatore data la Soluzione del Duale\n",
    "\n",
    "\n",
    "Data la soluzione $\\boldsymbol{\\alpha}^*$ del problema duale, questa determina la soluzione $(\\boldsymbol{w}^*,b^*)$ del problema primale. In particolare, è importante fare le seguenti osservazioni.\n",
    "\n",
    "**Osservazioni:** \n",
    "1. sia $\\boldsymbol{x}_i\\in X_{\\mathcal{T}}$ *NON* un support vector (lontano dal margine) per l'iperpiano separatore ottimale, cioè per $\\Pi_{\\boldsymbol{w}^*,b^*}$; allora l'elemento $i$-esimo di $\\boldsymbol{\\alpha}^*$ è nullo, cioè $\\alpha_i^*=0$.\n",
    "\n",
    "2. sia l'elemento $i$-esimo di $\\boldsymbol{\\alpha}^*$ non è nullo, cioè $\\alpha_i^*\\neq0$; allora $\\boldsymbol{x}_i\\in X_{\\mathcal{T}}$ è un support vector per l'iperpiano separatore ottimale, cioè per $\\Pi_{\\boldsymbol{w}^*,b^*}$ (nell'immagine solo i 3 vicini al margine).\n",
    "\n",
    "**Definizione/Notazione:** indichiamo con $\\mathcal{I}_{\\mathrm{sv}}$ l'insieme di indici corrispondenti agli elementi non nulli di $\\boldsymbol{\\alpha}^*$, cioè $$\\mathcal{I}_{\\mathrm{sv}} = \\{i \\ | \\ \\alpha_i^*\\neq 0\\}\\,;$$\n",
    "per l'osservazione (2) precedente abbiamo quindi che $\\boldsymbol{x}_i\\in X_{\\mathcal{T}}$ è un *support vector* di $\\Pi_{\\boldsymbol{w}^*,b^*}$ se $i\\in\\mathcal{I}_{\\mathrm{sv}}$.\n",
    "\n",
    "**Calcolo di** $\\Pi_{\\boldsymbol{w}^*,b^*}$**:**\n",
    "- $\\boldsymbol{w}^* = \\sum_{i=1}^T y_i\\alpha_i^* \\boldsymbol{x}_i = \\sum_{i\\in\\mathcal{I}_{\\mathrm{sv}}} y_i\\alpha_i^* \\boldsymbol{x}_i$.\n",
    "\n",
    "- $b^* = \\frac{1}{|\\mathcal{I}_{\\mathrm{sv}}|} \\sum_{i\\in\\mathcal{I}_{\\mathrm{sv}}} (y_i - \\boldsymbol{w}^{*\\,\\top} \\boldsymbol{x}_i)$\n",
    "\n",
    "### Bordi del Margine dell'Iperpiano Separatore Ottimale\n",
    "\n",
    "i bordi del margine $M_{\\boldsymbol{w}^*,b^*}$ sono identificati dai due iperpiani\n",
    "\\begin{equation}\n",
    "\\mathcal{M}^+_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\boldsymbol{w}^{*\\,\\top}\\boldsymbol{x} + b^* - 1 = 0\\}\n",
    "\\end{equation}\n",
    "e\n",
    "\\begin{equation}\n",
    "\\mathcal{M}^-_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\boldsymbol{w}^{*\\,\\top}\\boldsymbol{x} + b^* + 1 = 0\\}\\,;\n",
    "\\end{equation}\n",
    "cioè sono identificati rispettivamente da quegli $\\boldsymbol{x}\\in\\mathbb{R}^n$ tali che $\\boldsymbol{w}^{*\\,\\top}\\boldsymbol{x} + b^* =\\pm 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-knowing",
   "metadata": {},
   "source": [
    "## SVM con Margine \"Morbido\" (\"Soft\")\n",
    "\n",
    "Non sempre due classi sono perfettamente linearmente separabili. A causa di rumore sui dati, può infatti capitare che classi teoricamente linearmente separabili non lo siano nella pratica (escludiamo il caso di separazioni altamente non lineari).\n",
    "\n",
    "Diventa quindi necessario introdurre una certa tolleranza sulla possibilità di *oltrepassare il margine* per i vettori $\\boldsymbol{x}_i$.\n",
    "\n",
    "**Notazione/Definizione:** $\\delta_i$ indica la distanza di $\\boldsymbol{x}_i$ dal bordo di margine corrispondente alla sua classe, se $\\boldsymbol{x}_i$ ha superato questo bordo, altrimenti $\\delta_i = 0$ (vedi figura).\n",
    "\n",
    "<img src=\"figures/svm_soft_distances.png\" width=\"480\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-tomorrow",
   "metadata": {},
   "source": [
    "### Riformulazione del Problema di Minimizzazione\n",
    "\n",
    "Questo \"rilassamento\" delle condizioni per il margine, si traduce nell'introduzione di variabili di slack $\\xi_i$ nel problema primale, le quali rappresentano le \"distanze\" $\\delta_i$ moltiplicate per $||\\boldsymbol{w}||$, cioè: $\\xi_i = \\delta_i||\\boldsymbol{w}||$, per ogni $i=1,\\ldots ,T$.\n",
    "\n",
    "Il problema si Primale diventa quindi\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\min_{\\boldsymbol{w}} \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^T \\xi_i\\\\\n",
    "y_i (\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\geq 1 - \\xi_i \\,,\\quad & \\forall \\ i=1,\\ldots ,T\\\\\n",
    "\\xi_i\\geq 0\\,,\\quad & \\forall \\ i=1,\\ldots ,T\\\\\n",
    "\\end{cases}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "oppure (equivalentemente)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\min_{\\boldsymbol{w}} \\frac{1}{2} \\left( \\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^T \\xi_i^2 \\right)\\\\\n",
    "y_i (\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\geq 1 - \\xi_i \\,,\\quad & \\forall \\ i=1,\\ldots ,T\\\\\n",
    "\\xi_i\\geq 0\\,,\\quad & \\forall \\ i=1,\\ldots ,T\\\\\n",
    "\\end{cases}\\,;\n",
    "\\end{equation}\n",
    "\n",
    "L'iperparametro $C\\in\\mathbb{R}^+$ è un *parametro di regolarizzazione* che caratterizza il rilassamento delle condizioni per il margine:\n",
    "- $C\\rightarrow 0$ aumenta la \"*morbidezza*\" del margine, permettendo ai vettori $\\boldsymbol{x}_i$ di superarlo illimitatamente;\n",
    "- $C\\rightarrow +\\infty$ aumenta la \"*durezza*\" del margine, permettendo ai vettori $\\boldsymbol{x}_i$ di superarlo impercettibilmente;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-novel",
   "metadata": {},
   "source": [
    "# Esercitazione: Implementazione di SVM Lineari\n",
    "\n",
    "Nell'esercitazione di oggi utilizzeremo la classe *LinearSVC* di scikit-learn, pensata per l'implementazione di SVM lineari con *margine morbido*.\n",
    "\n",
    "**N.B.:** in scikit-learn non è prevista una classe per SVM lineari senza margine morbido. Per simulare l'assenza di margine morbido è sempre possibile scegliere valori di $C$ molto alti.\n",
    "\n",
    "**FILE DA SCARICARE:** per le seguenti celle di codice è necessario avere scaricato dalla pagina del corso il modulo *linear_r2.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifty-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOTA BENE! *****\n",
    "# perché %matplotlib widget funzioni, installare nell'ambiente virtuale \n",
    "# il pacchetto ipympl con il comando:\n",
    "# pip install ipympl\n",
    "#\n",
    "# ATTENZIONE: perché funzioni è necessario chiudere e rilanciare jupyter-lab\n",
    "#\n",
    "# STILE DI VISUALIZZAZIONE PLOT FATTI CON MATPLOTLIB\n",
    "%matplotlib widget\n",
    "#\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC  #anche la classe SVC potrebbe funzionare per le lineari\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from linear_r2 import generate_square, HyperplaneR2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-functionality",
   "metadata": {},
   "source": [
    "## Generazione Training Set\n",
    "\n",
    "Generiamo un training set artificiale per esercitarci con le SVM. La generazione si basa sulle funzioni definite nel modulo *linear_r2.py*.\n",
    "Generiamo un numero arbitrario e casuale di punti $\\boldsymbol{x}_i\\in[0,1]^2\\subset\\mathbb{R}^2$, separati nelle classi $\\pm 1$ dalla retta $r: x_2 = 0.35 \\cdot x_1 + 0.30$, equivalente all'iperpiano $[0.35, -1][x_1, x_2]^\\top + 0.30 = 0$.\n",
    "\n",
    "**Esercizio:** guarda il contenuto del modulo *linear_r2.py* per capire il codice della cella seguente e completa le parti mancanti per eseguire il plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electrical-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7671baac8cbd479b97c42d886aa03c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SEP. LINEARE + RUMORE')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 20210429\n",
    "n_samples = 50\n",
    "\n",
    "w_true = np.array([0.35, -1.])  #pesi\n",
    "b_true = 0.30\n",
    "\n",
    "line_true = HyperplaneR2(w_true, b_true) \n",
    "\n",
    "# Generazione dei dei punti nel quadrato [0, 1]^2\n",
    "X = generate_square(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Calcoliamo la classe di appartenenza dei punti rispetto alla retta sopra definita.\n",
    "\n",
    "# Caso senza rumore\n",
    "_, y = line_true.demiplane_evaluate(X)\n",
    "# Caso con rumore\n",
    "_, y_noise = line_true.demiplane_evaluate_noise(X)\n",
    "\n",
    "# Plot a confronto\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "# Plot del caso senza rumore\n",
    "axs[0].scatter(X[:,0],X[:,1], c=y)\n",
    "axs[0].plot([0., 1.], [line_true.line_x2(0.), line_true.line_x2(1.)])  #plot \n",
    "axs[0].set_title('SEP. LINEARE')\n",
    "# Plot del caso con rumore\n",
    "axs[1].scatter(X[:,0],X[:,1], c=y_noise)\n",
    "axs[1].plot([0., 1.], [line_true.line_x2(0.), line_true.line_x2(1.)])\n",
    "axs[1].set_title('SEP. LINEARE + RUMORE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-factory",
   "metadata": {},
   "source": [
    "### Utilizzo della Classe LinearSVC\n",
    "\n",
    "**Esercizio:** Leggere la documentazione della classe *LinearSVC* (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) e completare i codici delle celle seguenti.\n",
    "\n",
    "**Suggerimenti/Indicazioni l'esercizio:**\n",
    "1. per lo svolgimento dell'esercizio focalizzarsi sui parametri di inizializzazione *C, loss, dual, random_state*;\n",
    "2. il parametro di inizializzazione *loss* indica se la funzione obiettivo da minimizzare è $\\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w} + C\\sum_{i=1}^T\\xi_i$ (*hinge*) oppure $\\frac{1}{2}\\left(\\boldsymbol{w}^\\top\\boldsymbol{w} + C\\sum_{i=1}^T\\xi_i^2\\right)$ (*squared_hinge*);\n",
    "3. leggere bene i metodi della classe e gli attributi *coef_* ed *intercept_*;\n",
    "4. *solo per informazione:* il parametro *penalty='l1'* indica di usare $||\\boldsymbol{w}||_1= |w_1|+\\cdots +|w_n|$ invece del \"normale\" $||\\boldsymbol{w}||^2 = w_1^2 + \\cdots + w_2^2$ nella funzione obiettivo (caso *penalty='l2'*).\n",
    "\n",
    "**ATTENZIONE:** nella documentazione sembra indicato che la *hinge* loss non è compatibile con il parametro *dual=False*.\n",
    "\n",
    "**RICORDA:** dati $\\boldsymbol{w}^*$ e $b^*$ parametri dell'iperpiano separatore ottimale $\\Pi_{\\boldsymbol{w}^*,b^*}$, i bordi del margine sono identificati dai due iperpiani\n",
    "\\begin{equation}\n",
    "\\mathcal{M}^+_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\boldsymbol{w}^{*\\,\\top}\\boldsymbol{x} + b^* - 1 = 0\\}\n",
    "\\end{equation}\n",
    "e\n",
    "\\begin{equation}\n",
    "\\mathcal{M}^-_{\\boldsymbol{w}^*,b^*} = \\{\\boldsymbol{x}\\in\\mathbb{R}^n \\ | \\ \\boldsymbol{w}^{*\\,\\top}\\boldsymbol{x} + b^* + 1 = 0\\}\\,;\n",
    "\\end{equation}\n",
    "cioè sono identificati rispettivamente da quegli $\\boldsymbol{x}\\in\\mathbb{R}^n$ tali che $\\boldsymbol{w}^{*\\,\\top}\\boldsymbol{x} + b^* =\\pm 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suspended-kitchen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05f5c20009a414a83b7a4afc83fd667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SVM SEP. LINEARE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso SVM \"base\" immagino che le classi siano linearmente separabili\n",
    "\n",
    "C_hard = 1e10\n",
    "loss = 'squared_hinge'\n",
    "dual = False #Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features\n",
    "\n",
    "# Inizializzazione SVM\n",
    "lsvm_hard = LinearSVC(loss=loss, dual=dual, C=C_hard, random_state=random_state)\n",
    "\n",
    "# Addestramento SVM\n",
    "lsvm_hard.fit(X,y)\n",
    "\n",
    "# Pesi retta (iperpiano di R^2) separatrice\n",
    "w_hard = lsvm_hard.coef_\n",
    "b_hard = lsvm_hard.intercept_\n",
    "\n",
    "# Inizializzazione retta separatrice\n",
    "line_hard = HyperplaneR2(w_hard, b_hard)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "# Raffigurazione iperpiano separatore\n",
    "plt.plot([0.,1.], [line_hard.line_x2(0.), line_hard.line_x2(1.)], label='sep. hyperplane (svm)')\n",
    "# Raffigurazione margini\n",
    "plt.plot([0., 1.], [line_hard.margin_x2(0.)[0], line_hard.margin_x2(1.)[0]], 'r--', label='margin border')\n",
    "plt.plot([0., 1.], [line_hard.margin_x2(0.)[1], line_hard.margin_x2(1.)[1]], 'r--')\n",
    "# Raffigurazione vera retta separatrice\n",
    "plt.plot([0.,1.],[line_true.line_x2(0.), line_true.line_x2(1.)], label='true sep.')\n",
    "plt.legend()\n",
    "plt.title('SVM SEP. LINEARE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immune-excess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e133b537fc244580ad4e848f9d99ee45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SEP. LINEARE - C=2.5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso SVM con margini morbidi quando le classi non sono molto separate conviene\n",
    "\n",
    "#support vector tutti quelli dentro al margine (liberi) e quelli sul bordo (non liberi)\n",
    "\n",
    "C_soft1 = 10.\n",
    "C_soft2 = 2.5\n",
    "\n",
    "# Inizializzazione SVM\n",
    "lsvm_soft1 = LinearSVC(loss=loss, dual=dual, C=C_soft1)\n",
    "lsvm_soft2 = LinearSVC(loss=loss, dual=dual, C=C_soft2)\n",
    "\n",
    "# Addestramento SVM\n",
    "lsvm_soft1.fit(X,y)\n",
    "lsvm_soft2.fit(X,y)\n",
    "\n",
    "\n",
    "# Inizializzazione rette separatrici\n",
    "line_soft1 = HyperplaneR2(lsvm_soft1.coef_, lsvm_soft1.intercept_)\n",
    "line_soft2 = HyperplaneR2(lsvm_soft2.coef_, lsvm_soft2.intercept_)\n",
    "\n",
    "# Plot a confronto per i diversi valori di C\n",
    "fig_soft, axs_soft = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axs_soft[0].scatter(X[:, 0], X[:, 1], c=y_noise)\n",
    "axs_soft[0].plot([0.,1.], [line_soft1.line_x2(0.), line_soft1.line_x2(1.)], label='sep. hyperplane (svm)')\n",
    "axs_soft[0].plot([0., 1.], [line_soft1.margin_x2(0.)[0], line_soft1.margin_x2(1.)[0]], 'r--', label='margin border')\n",
    "axs_soft[0].plot([0., 1.], [line_soft1.margin_x2(0.)[1], line_soft1.margin_x2(1.)[1]], 'r--')\n",
    "axs_soft[0].legend()\n",
    "axs_soft[0].set_title('SEP. LINEARE - C={}'.format(C_soft1))\n",
    "axs_soft[1].scatter(X[:, 0], X[:, 1], c=y_noise)\n",
    "axs_soft[1].plot([0.,1.], [line_soft2.line_x2(0.), line_soft2.line_x2(1.)], label='sep. hyperplane (svm)')\n",
    "axs_soft[1].plot([0., 1.], [line_soft2.margin_x2(0.)[0], line_soft2.margin_x2(1.)[0]], 'r--', label='margin border')\n",
    "axs_soft[1].plot([0., 1.], [line_soft2.margin_x2(0.)[1], line_soft2.margin_x2(1.)[1]], 'r--')\n",
    "axs_soft[1].legend()\n",
    "axs_soft[1].set_title('SEP. LINEARE - C={}'.format(C_soft2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "italian-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axs_soft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-prevention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
